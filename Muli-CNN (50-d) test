
Max answerlength: 100
Vocabulary size: 1887
(9303, 100) (2326, 100)
Model: "model_15"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_43 (InputLayer)           (None, 100)          0                                            
__________________________________________________________________________________________________
input_44 (InputLayer)           (None, 100)          0                                            
__________________________________________________________________________________________________
input_45 (InputLayer)           (None, 100)          0                                            
__________________________________________________________________________________________________
embedding_43 (Embedding)        (None, 100, 50)      94350       input_43[0][0]                   
__________________________________________________________________________________________________
embedding_44 (Embedding)        (None, 100, 50)      94350       input_44[0][0]                   
__________________________________________________________________________________________________
embedding_45 (Embedding)        (None, 100, 50)      94350       input_45[0][0]                   
__________________________________________________________________________________________________
conv1d_43 (Conv1D)              (None, 99, 300)      30300       embedding_43[0][0]               
__________________________________________________________________________________________________
conv1d_44 (Conv1D)              (None, 98, 300)      45300       embedding_44[0][0]               
__________________________________________________________________________________________________
conv1d_45 (Conv1D)              (None, 97, 300)      60300       embedding_45[0][0]               
__________________________________________________________________________________________________
batch_normalization_113 (BatchN (None, 99, 300)      1200        conv1d_43[0][0]                  
__________________________________________________________________________________________________
batch_normalization_115 (BatchN (None, 98, 300)      1200        conv1d_44[0][0]                  
__________________________________________________________________________________________________
batch_normalization_117 (BatchN (None, 97, 300)      1200        conv1d_45[0][0]                  
__________________________________________________________________________________________________
dropout_113 (Dropout)           (None, 99, 300)      0           batch_normalization_113[0][0]    
__________________________________________________________________________________________________
dropout_115 (Dropout)           (None, 98, 300)      0           batch_normalization_115[0][0]    
__________________________________________________________________________________________________
dropout_117 (Dropout)           (None, 97, 300)      0           batch_normalization_117[0][0]    
__________________________________________________________________________________________________
max_pooling1d_43 (MaxPooling1D) (None, 1, 300)       0           dropout_113[0][0]                
__________________________________________________________________________________________________
max_pooling1d_44 (MaxPooling1D) (None, 1, 300)       0           dropout_115[0][0]                
__________________________________________________________________________________________________
max_pooling1d_45 (MaxPooling1D) (None, 1, 300)       0           dropout_117[0][0]                
__________________________________________________________________________________________________
batch_normalization_114 (BatchN (None, 1, 300)       1200        max_pooling1d_43[0][0]           
__________________________________________________________________________________________________
batch_normalization_116 (BatchN (None, 1, 300)       1200        max_pooling1d_44[0][0]           
__________________________________________________________________________________________________
batch_normalization_118 (BatchN (None, 1, 300)       1200        max_pooling1d_45[0][0]           
__________________________________________________________________________________________________
dropout_114 (Dropout)           (None, 1, 300)       0           batch_normalization_114[0][0]    
__________________________________________________________________________________________________
dropout_116 (Dropout)           (None, 1, 300)       0           batch_normalization_116[0][0]    
__________________________________________________________________________________________________
dropout_118 (Dropout)           (None, 1, 300)       0           batch_normalization_118[0][0]    
__________________________________________________________________________________________________
flatten_43 (Flatten)            (None, 300)          0           dropout_114[0][0]                
__________________________________________________________________________________________________
flatten_44 (Flatten)            (None, 300)          0           dropout_116[0][0]                
__________________________________________________________________________________________________
flatten_45 (Flatten)            (None, 300)          0           dropout_118[0][0]                
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 900)          0           flatten_43[0][0]                 
                                                                 flatten_44[0][0]                 
                                                                 flatten_45[0][0]                 
__________________________________________________________________________________________________
dense_43 (Dense)                (None, 200)          180200      concatenate_15[0][0]             
__________________________________________________________________________________________________
batch_normalization_119 (BatchN (None, 200)          800         dense_43[0][0]                   
__________________________________________________________________________________________________
dropout_119 (Dropout)           (None, 200)          0           batch_normalization_119[0][0]    
__________________________________________________________________________________________________
dense_44 (Dense)                (None, 100)          20100       dropout_119[0][0]                
__________________________________________________________________________________________________
batch_normalization_120 (BatchN (None, 100)          400         dense_44[0][0]                   
__________________________________________________________________________________________________
dropout_120 (Dropout)           (None, 100)          0           batch_normalization_120[0][0]    
__________________________________________________________________________________________________
dense_45 (Dense)                (None, 1)            101         dropout_120[0][0]                
==================================================================================================
Total params: 627,751
Trainable params: 623,551
Non-trainable params: 4,200
__________________________________________________________________________________________________
None
Train on 9303 samples, validate on 2326 samples
Epoch 1/350
9303/9303 [==============================] - 8s 841us/step - loss: 1.0449 - correlation_coefficient: 0.0713 - val_loss: 1.4250 - val_correlation_coefficient: 0.0500

Epoch 00001: loss improved from inf to 1.04495, saving model to bestweights4cnn50d
Epoch 2/350
9303/9303 [==============================] - 4s 394us/step - loss: 0.4232 - correlation_coefficient: 0.0813 - val_loss: 1.9960 - val_correlation_coefficient: 0.0660

Epoch 00002: loss improved from 1.04495 to 0.42318, saving model to bestweights4cnn50d
Epoch 3/350
9303/9303 [==============================] - 4s 397us/step - loss: 0.1981 - correlation_coefficient: 0.0918 - val_loss: 0.4651 - val_correlation_coefficient: 0.0907

Epoch 00003: loss improved from 0.42318 to 0.19815, saving model to bestweights4cnn50d
Epoch 4/350
9303/9303 [==============================] - 4s 399us/step - loss: 0.1189 - correlation_coefficient: 0.0958 - val_loss: 0.0880 - val_correlation_coefficient: 0.0975

Epoch 00004: loss improved from 0.19815 to 0.11891, saving model to bestweights4cnn50d
Epoch 5/350
9303/9303 [==============================] - 4s 398us/step - loss: 0.0818 - correlation_coefficient: 0.0969 - val_loss: 0.0767 - val_correlation_coefficient: 0.0987

Epoch 00005: loss improved from 0.11891 to 0.08175, saving model to bestweights4cnn50d
Epoch 6/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0576 - correlation_coefficient: 0.1003 - val_loss: 0.0560 - val_correlation_coefficient: 0.1036

Epoch 00006: loss improved from 0.08175 to 0.05759, saving model to bestweights4cnn50d
Epoch 7/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0423 - correlation_coefficient: 0.1054 - val_loss: 0.0303 - val_correlation_coefficient: 0.1105

Epoch 00007: loss improved from 0.05759 to 0.04231, saving model to bestweights4cnn50d
Epoch 8/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0347 - correlation_coefficient: 0.1130 - val_loss: 0.0311 - val_correlation_coefficient: 0.1182

Epoch 00008: loss improved from 0.04231 to 0.03467, saving model to bestweights4cnn50d
Epoch 9/350
9303/9303 [==============================] - 4s 392us/step - loss: 0.0273 - correlation_coefficient: 0.1230 - val_loss: 0.0224 - val_correlation_coefficient: 0.1293

Epoch 00009: loss improved from 0.03467 to 0.02734, saving model to bestweights4cnn50d
Epoch 10/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0223 - correlation_coefficient: 0.1353 - val_loss: 0.0293 - val_correlation_coefficient: 0.1420

Epoch 00010: loss improved from 0.02734 to 0.02232, saving model to bestweights4cnn50d
Epoch 11/350
9303/9303 [==============================] - 4s 392us/step - loss: 0.0185 - correlation_coefficient: 0.1496 - val_loss: 0.0173 - val_correlation_coefficient: 0.1565

Epoch 00011: loss improved from 0.02232 to 0.01853, saving model to bestweights4cnn50d
Epoch 12/350
9303/9303 [==============================] - 4s 379us/step - loss: 0.0158 - correlation_coefficient: 0.1631 - val_loss: 0.0119 - val_correlation_coefficient: 0.1702

Epoch 00012: loss improved from 0.01853 to 0.01584, saving model to bestweights4cnn50d
Epoch 13/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0140 - correlation_coefficient: 0.1770 - val_loss: 0.0125 - val_correlation_coefficient: 0.1838

Epoch 00013: loss improved from 0.01584 to 0.01399, saving model to bestweights4cnn50d
Epoch 14/350
9303/9303 [==============================] - 3s 375us/step - loss: 0.0119 - correlation_coefficient: 0.1904 - val_loss: 0.0119 - val_correlation_coefficient: 0.1974

Epoch 00014: loss improved from 0.01399 to 0.01189, saving model to bestweights4cnn50d
Epoch 15/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0105 - correlation_coefficient: 0.2044 - val_loss: 0.0161 - val_correlation_coefficient: 0.2109

Epoch 00015: loss improved from 0.01189 to 0.01049, saving model to bestweights4cnn50d
Epoch 16/350
9303/9303 [==============================] - 4s 378us/step - loss: 0.0097 - correlation_coefficient: 0.2176 - val_loss: 0.0108 - val_correlation_coefficient: 0.2240

Epoch 00016: loss improved from 0.01049 to 0.00965, saving model to bestweights4cnn50d
Epoch 17/350
9303/9303 [==============================] - 4s 403us/step - loss: 0.0085 - correlation_coefficient: 0.2309 - val_loss: 0.0129 - val_correlation_coefficient: 0.2371

Epoch 00017: loss improved from 0.00965 to 0.00852, saving model to bestweights4cnn50d
Epoch 18/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0075 - correlation_coefficient: 0.2442 - val_loss: 0.0080 - val_correlation_coefficient: 0.2503

Epoch 00018: loss improved from 0.00852 to 0.00749, saving model to bestweights4cnn50d
Epoch 19/350
9303/9303 [==============================] - 4s 394us/step - loss: 0.0072 - correlation_coefficient: 0.2570 - val_loss: 0.0093 - val_correlation_coefficient: 0.2628

Epoch 00019: loss improved from 0.00749 to 0.00723, saving model to bestweights4cnn50d
Epoch 20/350
9303/9303 [==============================] - 4s 386us/step - loss: 0.0065 - correlation_coefficient: 0.2690 - val_loss: 0.0096 - val_correlation_coefficient: 0.2748

Epoch 00020: loss improved from 0.00723 to 0.00647, saving model to bestweights4cnn50d
Epoch 21/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0063 - correlation_coefficient: 0.2812 - val_loss: 0.0109 - val_correlation_coefficient: 0.2865

Epoch 00021: loss improved from 0.00647 to 0.00627, saving model to bestweights4cnn50d
Epoch 22/350
9303/9303 [==============================] - 4s 397us/step - loss: 0.0057 - correlation_coefficient: 0.2925 - val_loss: 0.0058 - val_correlation_coefficient: 0.2978

Epoch 00022: loss improved from 0.00627 to 0.00568, saving model to bestweights4cnn50d
Epoch 23/350
9303/9303 [==============================] - 4s 394us/step - loss: 0.0056 - correlation_coefficient: 0.3036 - val_loss: 0.0050 - val_correlation_coefficient: 0.3088

Epoch 00023: loss improved from 0.00568 to 0.00563, saving model to bestweights4cnn50d
Epoch 24/350
9303/9303 [==============================] - 4s 395us/step - loss: 0.0051 - correlation_coefficient: 0.3144 - val_loss: 0.0067 - val_correlation_coefficient: 0.3194

Epoch 00024: loss improved from 0.00563 to 0.00505, saving model to bestweights4cnn50d
Epoch 25/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0050 - correlation_coefficient: 0.3250 - val_loss: 0.0046 - val_correlation_coefficient: 0.3298

Epoch 00025: loss improved from 0.00505 to 0.00498, saving model to bestweights4cnn50d
Epoch 26/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0047 - correlation_coefficient: 0.3350 - val_loss: 0.0058 - val_correlation_coefficient: 0.3398

Epoch 00026: loss improved from 0.00498 to 0.00469, saving model to bestweights4cnn50d
Epoch 27/350
9303/9303 [==============================] - 4s 395us/step - loss: 0.0048 - correlation_coefficient: 0.3452 - val_loss: 0.0086 - val_correlation_coefficient: 0.3497

Epoch 00027: loss did not improve from 0.00469
Epoch 28/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0044 - correlation_coefficient: 0.3547 - val_loss: 0.0070 - val_correlation_coefficient: 0.3591

Epoch 00028: loss improved from 0.00469 to 0.00444, saving model to bestweights4cnn50d
Epoch 29/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0042 - correlation_coefficient: 0.3641 - val_loss: 0.0070 - val_correlation_coefficient: 0.3681

Epoch 00029: loss improved from 0.00444 to 0.00415, saving model to bestweights4cnn50d
Epoch 30/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0040 - correlation_coefficient: 0.3726 - val_loss: 0.0044 - val_correlation_coefficient: 0.3768

Epoch 00030: loss improved from 0.00415 to 0.00397, saving model to bestweights4cnn50d
Epoch 31/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0040 - correlation_coefficient: 0.3812 - val_loss: 0.0052 - val_correlation_coefficient: 0.3852

Epoch 00031: loss did not improve from 0.00397
Epoch 32/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0040 - correlation_coefficient: 0.3894 - val_loss: 0.0067 - val_correlation_coefficient: 0.3931

Epoch 00032: loss did not improve from 0.00397
Epoch 33/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0041 - correlation_coefficient: 0.3969 - val_loss: 0.0030 - val_correlation_coefficient: 0.4007

Epoch 00033: loss did not improve from 0.00397
Epoch 34/350
9303/9303 [==============================] - 4s 393us/step - loss: 0.0035 - correlation_coefficient: 0.4050 - val_loss: 0.0050 - val_correlation_coefficient: 0.4086

Epoch 00034: loss improved from 0.00397 to 0.00354, saving model to bestweights4cnn50d
Epoch 35/350
9303/9303 [==============================] - 4s 386us/step - loss: 0.0042 - correlation_coefficient: 0.4123 - val_loss: 0.0074 - val_correlation_coefficient: 0.4157

Epoch 00035: loss did not improve from 0.00354
Epoch 36/350
9303/9303 [==============================] - 4s 403us/step - loss: 0.0036 - correlation_coefficient: 0.4198 - val_loss: 0.0037 - val_correlation_coefficient: 0.4232

Epoch 00036: loss did not improve from 0.00354
Epoch 37/350
9303/9303 [==============================] - 3s 372us/step - loss: 0.0038 - correlation_coefficient: 0.4272 - val_loss: 0.0046 - val_correlation_coefficient: 0.4304

Epoch 00037: loss did not improve from 0.00354
Epoch 38/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0034 - correlation_coefficient: 0.4342 - val_loss: 0.0028 - val_correlation_coefficient: 0.4375

Epoch 00038: loss improved from 0.00354 to 0.00340, saving model to bestweights4cnn50d
Epoch 39/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0037 - correlation_coefficient: 0.4411 - val_loss: 0.0033 - val_correlation_coefficient: 0.4443

Epoch 00039: loss did not improve from 0.00340
Epoch 40/350
9303/9303 [==============================] - 4s 378us/step - loss: 0.0038 - correlation_coefficient: 0.4481 - val_loss: 0.0021 - val_correlation_coefficient: 0.4512

Epoch 00040: loss did not improve from 0.00340
Epoch 41/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0035 - correlation_coefficient: 0.4546 - val_loss: 0.0036 - val_correlation_coefficient: 0.4576

Epoch 00041: loss did not improve from 0.00340
Epoch 42/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0032 - correlation_coefficient: 0.4610 - val_loss: 0.0062 - val_correlation_coefficient: 0.4639

Epoch 00042: loss improved from 0.00340 to 0.00319, saving model to bestweights4cnn50d
Epoch 43/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0036 - correlation_coefficient: 0.4671 - val_loss: 0.0075 - val_correlation_coefficient: 0.4698

Epoch 00043: loss did not improve from 0.00319
Epoch 44/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0033 - correlation_coefficient: 0.4728 - val_loss: 0.0046 - val_correlation_coefficient: 0.4755

Epoch 00044: loss did not improve from 0.00319
Epoch 45/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0032 - correlation_coefficient: 0.4786 - val_loss: 0.0036 - val_correlation_coefficient: 0.4813

Epoch 00045: loss did not improve from 0.00319
Epoch 46/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0033 - correlation_coefficient: 0.4841 - val_loss: 0.0055 - val_correlation_coefficient: 0.4867

Epoch 00046: loss did not improve from 0.00319
Epoch 47/350
9303/9303 [==============================] - 4s 379us/step - loss: 0.0034 - correlation_coefficient: 0.4897 - val_loss: 0.0036 - val_correlation_coefficient: 0.4922

Epoch 00047: loss did not improve from 0.00319
Epoch 48/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0032 - correlation_coefficient: 0.4951 - val_loss: 0.0029 - val_correlation_coefficient: 0.4976

Epoch 00048: loss did not improve from 0.00319
Epoch 49/350
9303/9303 [==============================] - 4s 393us/step - loss: 0.0030 - correlation_coefficient: 0.5004 - val_loss: 0.0028 - val_correlation_coefficient: 0.5028

Epoch 00049: loss improved from 0.00319 to 0.00299, saving model to bestweights4cnn50d
Epoch 50/350
9303/9303 [==============================] - 4s 377us/step - loss: 0.0031 - correlation_coefficient: 0.5057 - val_loss: 0.0026 - val_correlation_coefficient: 0.5080

Epoch 00050: loss did not improve from 0.00299
Epoch 51/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0035 - correlation_coefficient: 0.5107 - val_loss: 0.0025 - val_correlation_coefficient: 0.5130

Epoch 00051: loss did not improve from 0.00299
Epoch 52/350
9303/9303 [==============================] - 4s 379us/step - loss: 0.0034 - correlation_coefficient: 0.5156 - val_loss: 0.0022 - val_correlation_coefficient: 0.5179

Epoch 00052: loss did not improve from 0.00299
Epoch 53/350
9303/9303 [==============================] - 4s 399us/step - loss: 0.0032 - correlation_coefficient: 0.5206 - val_loss: 0.0025 - val_correlation_coefficient: 0.5228

Epoch 00053: loss did not improve from 0.00299
Epoch 54/350
9303/9303 [==============================] - 4s 455us/step - loss: 0.0029 - correlation_coefficient: 0.5254 - val_loss: 0.0019 - val_correlation_coefficient: 0.5275

Epoch 00054: loss improved from 0.00299 to 0.00293, saving model to bestweights4cnn50d
Epoch 55/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0031 - correlation_coefficient: 0.5301 - val_loss: 0.0067 - val_correlation_coefficient: 0.5321

Epoch 00055: loss did not improve from 0.00293
Epoch 56/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0032 - correlation_coefficient: 0.5345 - val_loss: 0.0045 - val_correlation_coefficient: 0.5365

Epoch 00056: loss did not improve from 0.00293
Epoch 57/350
9303/9303 [==============================] - 4s 377us/step - loss: 0.0029 - correlation_coefficient: 0.5386 - val_loss: 0.0018 - val_correlation_coefficient: 0.5407

Epoch 00057: loss improved from 0.00293 to 0.00291, saving model to bestweights4cnn50d
Epoch 58/350
9303/9303 [==============================] - 4s 378us/step - loss: 0.0028 - correlation_coefficient: 0.5431 - val_loss: 0.0015 - val_correlation_coefficient: 0.5452

Epoch 00058: loss improved from 0.00291 to 0.00282, saving model to bestweights4cnn50d
Epoch 59/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0027 - correlation_coefficient: 0.5477 - val_loss: 0.0028 - val_correlation_coefficient: 0.5496

Epoch 00059: loss improved from 0.00282 to 0.00267, saving model to bestweights4cnn50d
Epoch 60/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0027 - correlation_coefficient: 0.5518 - val_loss: 0.0054 - val_correlation_coefficient: 0.5537

Epoch 00060: loss did not improve from 0.00267
Epoch 61/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0027 - correlation_coefficient: 0.5559 - val_loss: 0.0042 - val_correlation_coefficient: 0.5578

Epoch 00061: loss did not improve from 0.00267
Epoch 62/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0028 - correlation_coefficient: 0.5598 - val_loss: 0.0031 - val_correlation_coefficient: 0.5616

Epoch 00062: loss did not improve from 0.00267
Epoch 63/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0030 - correlation_coefficient: 0.5637 - val_loss: 0.0064 - val_correlation_coefficient: 0.5654

Epoch 00063: loss did not improve from 0.00267
Epoch 64/350
9303/9303 [==============================] - 4s 378us/step - loss: 0.0028 - correlation_coefficient: 0.5672 - val_loss: 0.0014 - val_correlation_coefficient: 0.5691

Epoch 00064: loss did not improve from 0.00267
Epoch 65/350
9303/9303 [==============================] - 4s 378us/step - loss: 0.0028 - correlation_coefficient: 0.5712 - val_loss: 0.0027 - val_correlation_coefficient: 0.5730

Epoch 00065: loss did not improve from 0.00267
Epoch 66/350
9303/9303 [==============================] - 4s 394us/step - loss: 0.0028 - correlation_coefficient: 0.5750 - val_loss: 0.0022 - val_correlation_coefficient: 0.5767

Epoch 00066: loss did not improve from 0.00267
Epoch 67/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0027 - correlation_coefficient: 0.5787 - val_loss: 0.0032 - val_correlation_coefficient: 0.5803

Epoch 00067: loss did not improve from 0.00267
Epoch 68/350
9303/9303 [==============================] - 4s 403us/step - loss: 0.0027 - correlation_coefficient: 0.5822 - val_loss: 0.0012 - val_correlation_coefficient: 0.5839

Epoch 00068: loss did not improve from 0.00267
Epoch 69/350
9303/9303 [==============================] - 4s 392us/step - loss: 0.0029 - correlation_coefficient: 0.5859 - val_loss: 0.0020 - val_correlation_coefficient: 0.5875

Epoch 00069: loss did not improve from 0.00267
Epoch 70/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0027 - correlation_coefficient: 0.5894 - val_loss: 0.0018 - val_correlation_coefficient: 0.5910

Epoch 00070: loss did not improve from 0.00267
Epoch 71/350
9303/9303 [==============================] - 4s 378us/step - loss: 0.0027 - correlation_coefficient: 0.5929 - val_loss: 0.0024 - val_correlation_coefficient: 0.5944

Epoch 00071: loss did not improve from 0.00267
Epoch 72/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0029 - correlation_coefficient: 0.5962 - val_loss: 0.0013 - val_correlation_coefficient: 0.5977

Epoch 00072: loss did not improve from 0.00267
Epoch 73/350
9303/9303 [==============================] - 4s 385us/step - loss: 0.0027 - correlation_coefficient: 0.5995 - val_loss: 0.0027 - val_correlation_coefficient: 0.6010

Epoch 00073: loss did not improve from 0.00267
Epoch 74/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0026 - correlation_coefficient: 0.6028 - val_loss: 0.0028 - val_correlation_coefficient: 0.6043

Epoch 00074: loss improved from 0.00267 to 0.00260, saving model to bestweights4cnn50d
Epoch 75/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0027 - correlation_coefficient: 0.6059 - val_loss: 0.0024 - val_correlation_coefficient: 0.6073

Epoch 00075: loss did not improve from 0.00260
Epoch 76/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0026 - correlation_coefficient: 0.6090 - val_loss: 0.0022 - val_correlation_coefficient: 0.6104

Epoch 00076: loss improved from 0.00260 to 0.00257, saving model to bestweights4cnn50d
Epoch 77/350
9303/9303 [==============================] - 4s 386us/step - loss: 0.0025 - correlation_coefficient: 0.6120 - val_loss: 0.0027 - val_correlation_coefficient: 0.6134

Epoch 00077: loss improved from 0.00257 to 0.00251, saving model to bestweights4cnn50d
Epoch 78/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0030 - correlation_coefficient: 0.6150 - val_loss: 0.0012 - val_correlation_coefficient: 0.6164

Epoch 00078: loss did not improve from 0.00251
Epoch 79/350
9303/9303 [==============================] - 4s 402us/step - loss: 0.0027 - correlation_coefficient: 0.6182 - val_loss: 0.0012 - val_correlation_coefficient: 0.6196

Epoch 00079: loss did not improve from 0.00251
Epoch 80/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0025 - correlation_coefficient: 0.6211 - val_loss: 0.0015 - val_correlation_coefficient: 0.6225

Epoch 00080: loss improved from 0.00251 to 0.00250, saving model to bestweights4cnn50d
Epoch 81/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0023 - correlation_coefficient: 0.6241 - val_loss: 0.0022 - val_correlation_coefficient: 0.6254

Epoch 00081: loss improved from 0.00250 to 0.00232, saving model to bestweights4cnn50d
Epoch 82/350
9303/9303 [==============================] - 4s 392us/step - loss: 0.0022 - correlation_coefficient: 0.6269 - val_loss: 0.0020 - val_correlation_coefficient: 0.6282

Epoch 00082: loss improved from 0.00232 to 0.00223, saving model to bestweights4cnn50d
Epoch 83/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0024 - correlation_coefficient: 0.6297 - val_loss: 0.0035 - val_correlation_coefficient: 0.6309

Epoch 00083: loss did not improve from 0.00223
Epoch 84/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0024 - correlation_coefficient: 0.6323 - val_loss: 0.0021 - val_correlation_coefficient: 0.6336

Epoch 00084: loss did not improve from 0.00223
Epoch 85/350
9303/9303 [==============================] - 4s 394us/step - loss: 0.0021 - correlation_coefficient: 0.6351 - val_loss: 0.0015 - val_correlation_coefficient: 0.6364

Epoch 00085: loss improved from 0.00223 to 0.00207, saving model to bestweights4cnn50d
Epoch 86/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0023 - correlation_coefficient: 0.6379 - val_loss: 0.0026 - val_correlation_coefficient: 0.6391

Epoch 00086: loss did not improve from 0.00207
Epoch 87/350
9303/9303 [==============================] - 4s 397us/step - loss: 0.0024 - correlation_coefficient: 0.6404 - val_loss: 0.0014 - val_correlation_coefficient: 0.6416

Epoch 00087: loss did not improve from 0.00207
Epoch 88/350
9303/9303 [==============================] - 4s 406us/step - loss: 0.0025 - correlation_coefficient: 0.6430 - val_loss: 0.0023 - val_correlation_coefficient: 0.6442

Epoch 00088: loss did not improve from 0.00207
Epoch 89/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0023 - correlation_coefficient: 0.6455 - val_loss: 0.0018 - val_correlation_coefficient: 0.6467

Epoch 00089: loss did not improve from 0.00207
Epoch 90/350
9303/9303 [==============================] - 4s 394us/step - loss: 0.0024 - correlation_coefficient: 0.6480 - val_loss: 0.0011 - val_correlation_coefficient: 0.6492

Epoch 00090: loss did not improve from 0.00207
Epoch 91/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0023 - correlation_coefficient: 0.6506 - val_loss: 0.0014 - val_correlation_coefficient: 0.6518

Epoch 00091: loss did not improve from 0.00207
Epoch 92/350
9303/9303 [==============================] - 4s 377us/step - loss: 0.0022 - correlation_coefficient: 0.6531 - val_loss: 0.0026 - val_correlation_coefficient: 0.6542

Epoch 00092: loss did not improve from 0.00207
Epoch 93/350
9303/9303 [==============================] - 4s 397us/step - loss: 0.0020 - correlation_coefficient: 0.6555 - val_loss: 0.0013 - val_correlation_coefficient: 0.6566

Epoch 00093: loss improved from 0.00207 to 0.00201, saving model to bestweights4cnn50d
Epoch 94/350
9303/9303 [==============================] - 4s 399us/step - loss: 0.0021 - correlation_coefficient: 0.6579 - val_loss: 0.0016 - val_correlation_coefficient: 0.6590

Epoch 00094: loss did not improve from 0.00201
Epoch 95/350
9303/9303 [==============================] - 4s 386us/step - loss: 0.0021 - correlation_coefficient: 0.6602 - val_loss: 0.0018 - val_correlation_coefficient: 0.6613

Epoch 00095: loss did not improve from 0.00201
Epoch 96/350
9303/9303 [==============================] - 4s 379us/step - loss: 0.0022 - correlation_coefficient: 0.6625 - val_loss: 0.0014 - val_correlation_coefficient: 0.6635

Epoch 00096: loss did not improve from 0.00201
Epoch 97/350
9303/9303 [==============================] - 3s 376us/step - loss: 0.0022 - correlation_coefficient: 0.6647 - val_loss: 0.0019 - val_correlation_coefficient: 0.6658

Epoch 00097: loss did not improve from 0.00201
Epoch 98/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0025 - correlation_coefficient: 0.6669 - val_loss: 0.0020 - val_correlation_coefficient: 0.6679

Epoch 00098: loss did not improve from 0.00201
Epoch 99/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0022 - correlation_coefficient: 0.6691 - val_loss: 0.0010 - val_correlation_coefficient: 0.6701

Epoch 00099: loss did not improve from 0.00201
Epoch 100/350
9303/9303 [==============================] - 3s 375us/step - loss: 0.0021 - correlation_coefficient: 0.6713 - val_loss: 0.0010 - val_correlation_coefficient: 0.6723

Epoch 00100: loss did not improve from 0.00201
Epoch 101/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0024 - correlation_coefficient: 0.6735 - val_loss: 7.7562e-04 - val_correlation_coefficient: 0.6745

Epoch 00101: loss did not improve from 0.00201
Epoch 102/350
9303/9303 [==============================] - 4s 393us/step - loss: 0.0020 - correlation_coefficient: 0.6757 - val_loss: 0.0017 - val_correlation_coefficient: 0.6766

Epoch 00102: loss improved from 0.00201 to 0.00201, saving model to bestweights4cnn50d
Epoch 103/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0021 - correlation_coefficient: 0.6777 - val_loss: 0.0011 - val_correlation_coefficient: 0.6787

Epoch 00103: loss did not improve from 0.00201
Epoch 104/350
9303/9303 [==============================] - 4s 377us/step - loss: 0.0020 - correlation_coefficient: 0.6798 - val_loss: 0.0015 - val_correlation_coefficient: 0.6807

Epoch 00104: loss did not improve from 0.00201
Epoch 105/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0021 - correlation_coefficient: 0.6818 - val_loss: 0.0010 - val_correlation_coefficient: 0.6828

Epoch 00105: loss did not improve from 0.00201
Epoch 106/350
9303/9303 [==============================] - 3s 366us/step - loss: 0.0018 - correlation_coefficient: 0.6839 - val_loss: 0.0010 - val_correlation_coefficient: 0.6848

Epoch 00106: loss improved from 0.00201 to 0.00178, saving model to bestweights4cnn50d
Epoch 107/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0019 - correlation_coefficient: 0.6859 - val_loss: 0.0012 - val_correlation_coefficient: 0.6869

Epoch 00107: loss did not improve from 0.00178
Epoch 108/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0020 - correlation_coefficient: 0.6880 - val_loss: 6.1571e-04 - val_correlation_coefficient: 0.6889

Epoch 00108: loss did not improve from 0.00178
Epoch 109/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0018 - correlation_coefficient: 0.6900 - val_loss: 0.0011 - val_correlation_coefficient: 0.6909

Epoch 00109: loss did not improve from 0.00178
Epoch 110/350
9303/9303 [==============================] - 4s 379us/step - loss: 0.0019 - correlation_coefficient: 0.6919 - val_loss: 9.7576e-04 - val_correlation_coefficient: 0.6928

Epoch 00110: loss did not improve from 0.00178
Epoch 111/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0018 - correlation_coefficient: 0.6938 - val_loss: 8.6117e-04 - val_correlation_coefficient: 0.6947

Epoch 00111: loss did not improve from 0.00178
Epoch 112/350
9303/9303 [==============================] - 4s 405us/step - loss: 0.0018 - correlation_coefficient: 0.6958 - val_loss: 0.0013 - val_correlation_coefficient: 0.6966

Epoch 00112: loss did not improve from 0.00178
Epoch 113/350
9303/9303 [==============================] - 4s 377us/step - loss: 0.0018 - correlation_coefficient: 0.6977 - val_loss: 0.0023 - val_correlation_coefficient: 0.6985

Epoch 00113: loss improved from 0.00178 to 0.00177, saving model to bestweights4cnn50d
Epoch 114/350
9303/9303 [==============================] - 3s 375us/step - loss: 0.0018 - correlation_coefficient: 0.6994 - val_loss: 0.0013 - val_correlation_coefficient: 0.7002

Epoch 00114: loss did not improve from 0.00177
Epoch 115/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0019 - correlation_coefficient: 0.7012 - val_loss: 0.0020 - val_correlation_coefficient: 0.7020

Epoch 00115: loss did not improve from 0.00177
Epoch 116/350
9303/9303 [==============================] - 4s 378us/step - loss: 0.0018 - correlation_coefficient: 0.7029 - val_loss: 0.0027 - val_correlation_coefficient: 0.7036

Epoch 00116: loss did not improve from 0.00177
Epoch 117/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0019 - correlation_coefficient: 0.7045 - val_loss: 9.5545e-04 - val_correlation_coefficient: 0.7053

Epoch 00117: loss did not improve from 0.00177
Epoch 118/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0016 - correlation_coefficient: 0.7063 - val_loss: 0.0025 - val_correlation_coefficient: 0.7070

Epoch 00118: loss improved from 0.00177 to 0.00161, saving model to bestweights4cnn50d
Epoch 119/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0018 - correlation_coefficient: 0.7079 - val_loss: 8.4327e-04 - val_correlation_coefficient: 0.7087

Epoch 00119: loss did not improve from 0.00161
Epoch 120/350
9303/9303 [==============================] - 4s 386us/step - loss: 0.0019 - correlation_coefficient: 0.7096 - val_loss: 0.0013 - val_correlation_coefficient: 0.7104

Epoch 00120: loss did not improve from 0.00161
Epoch 121/350
9303/9303 [==============================] - 4s 378us/step - loss: 0.0016 - correlation_coefficient: 0.7113 - val_loss: 0.0029 - val_correlation_coefficient: 0.7120

Epoch 00121: loss did not improve from 0.00161
Epoch 122/350
9303/9303 [==============================] - 4s 377us/step - loss: 0.0016 - correlation_coefficient: 0.7128 - val_loss: 0.0015 - val_correlation_coefficient: 0.7136

Epoch 00122: loss did not improve from 0.00161
Epoch 123/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0016 - correlation_coefficient: 0.7144 - val_loss: 0.0021 - val_correlation_coefficient: 0.7152

Epoch 00123: loss improved from 0.00161 to 0.00156, saving model to bestweights4cnn50d
Epoch 124/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0014 - correlation_coefficient: 0.7160 - val_loss: 9.5154e-04 - val_correlation_coefficient: 0.7167

Epoch 00124: loss improved from 0.00156 to 0.00141, saving model to bestweights4cnn50d
Epoch 125/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0015 - correlation_coefficient: 0.7176 - val_loss: 0.0026 - val_correlation_coefficient: 0.7183

Epoch 00125: loss did not improve from 0.00141
Epoch 126/350
9303/9303 [==============================] - 4s 401us/step - loss: 0.0017 - correlation_coefficient: 0.7191 - val_loss: 0.0020 - val_correlation_coefficient: 0.7198

Epoch 00126: loss did not improve from 0.00141
Epoch 127/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0018 - correlation_coefficient: 0.7206 - val_loss: 0.0019 - val_correlation_coefficient: 0.7213

Epoch 00127: loss did not improve from 0.00141
Epoch 128/350
9303/9303 [==============================] - 4s 379us/step - loss: 0.0018 - correlation_coefficient: 0.7221 - val_loss: 9.5128e-04 - val_correlation_coefficient: 0.7228

Epoch 00128: loss did not improve from 0.00141
Epoch 129/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0018 - correlation_coefficient: 0.7236 - val_loss: 0.0015 - val_correlation_coefficient: 0.7243

Epoch 00129: loss did not improve from 0.00141
Epoch 130/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0017 - correlation_coefficient: 0.7251 - val_loss: 9.1267e-04 - val_correlation_coefficient: 0.7258

Epoch 00130: loss did not improve from 0.00141
Epoch 131/350
9303/9303 [==============================] - 4s 377us/step - loss: 0.0016 - correlation_coefficient: 0.7266 - val_loss: 0.0011 - val_correlation_coefficient: 0.7273

Epoch 00131: loss did not improve from 0.00141
Epoch 132/350
9303/9303 [==============================] - 4s 377us/step - loss: 0.0017 - correlation_coefficient: 0.7281 - val_loss: 0.0024 - val_correlation_coefficient: 0.7287

Epoch 00132: loss did not improve from 0.00141
Epoch 133/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0016 - correlation_coefficient: 0.7294 - val_loss: 4.2490e-04 - val_correlation_coefficient: 0.7301

Epoch 00133: loss did not improve from 0.00141
Epoch 134/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0017 - correlation_coefficient: 0.7309 - val_loss: 9.9113e-04 - val_correlation_coefficient: 0.7316

Epoch 00134: loss did not improve from 0.00141
Epoch 135/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0015 - correlation_coefficient: 0.7324 - val_loss: 9.7564e-04 - val_correlation_coefficient: 0.7331

Epoch 00135: loss did not improve from 0.00141
Epoch 136/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0014 - correlation_coefficient: 0.7338 - val_loss: 9.8152e-04 - val_correlation_coefficient: 0.7345

Epoch 00136: loss improved from 0.00141 to 0.00139, saving model to bestweights4cnn50d
Epoch 137/350
9303/9303 [==============================] - 4s 455us/step - loss: 0.0016 - correlation_coefficient: 0.7352 - val_loss: 0.0017 - val_correlation_coefficient: 0.7358

Epoch 00137: loss did not improve from 0.00139
Epoch 138/350
9303/9303 [==============================] - 4s 396us/step - loss: 0.0016 - correlation_coefficient: 0.7365 - val_loss: 4.0229e-04 - val_correlation_coefficient: 0.7372

Epoch 00138: loss did not improve from 0.00139
Epoch 139/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0017 - correlation_coefficient: 0.7380 - val_loss: 0.0010 - val_correlation_coefficient: 0.7386

Epoch 00139: loss did not improve from 0.00139
Epoch 140/350
9303/9303 [==============================] - 4s 379us/step - loss: 0.0016 - correlation_coefficient: 0.7393 - val_loss: 0.0023 - val_correlation_coefficient: 0.7399

Epoch 00140: loss did not improve from 0.00139
Epoch 141/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0017 - correlation_coefficient: 0.7406 - val_loss: 0.0017 - val_correlation_coefficient: 0.7412

Epoch 00141: loss did not improve from 0.00139
Epoch 142/350
9303/9303 [==============================] - 4s 401us/step - loss: 0.0016 - correlation_coefficient: 0.7418 - val_loss: 0.0012 - val_correlation_coefficient: 0.7424

Epoch 00142: loss did not improve from 0.00139
Epoch 143/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0016 - correlation_coefficient: 0.7431 - val_loss: 0.0013 - val_correlation_coefficient: 0.7437

Epoch 00143: loss did not improve from 0.00139
Epoch 144/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0013 - correlation_coefficient: 0.7444 - val_loss: 0.0012 - val_correlation_coefficient: 0.7450

Epoch 00144: loss improved from 0.00139 to 0.00133, saving model to bestweights4cnn50d
Epoch 145/350
9303/9303 [==============================] - 4s 397us/step - loss: 0.0014 - correlation_coefficient: 0.7457 - val_loss: 0.0012 - val_correlation_coefficient: 0.7463

Epoch 00145: loss did not improve from 0.00133
Epoch 146/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0014 - correlation_coefficient: 0.7469 - val_loss: 0.0020 - val_correlation_coefficient: 0.7475

Epoch 00146: loss did not improve from 0.00133
Epoch 147/350
9303/9303 [==============================] - 4s 379us/step - loss: 0.0015 - correlation_coefficient: 0.7481 - val_loss: 0.0014 - val_correlation_coefficient: 0.7487

Epoch 00147: loss did not improve from 0.00133
Epoch 148/350
9303/9303 [==============================] - 3s 376us/step - loss: 0.0014 - correlation_coefficient: 0.7494 - val_loss: 0.0013 - val_correlation_coefficient: 0.7499

Epoch 00148: loss did not improve from 0.00133
Epoch 149/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0013 - correlation_coefficient: 0.7506 - val_loss: 0.0019 - val_correlation_coefficient: 0.7511

Epoch 00149: loss improved from 0.00133 to 0.00129, saving model to bestweights4cnn50d
Epoch 150/350
9303/9303 [==============================] - 4s 397us/step - loss: 0.0013 - correlation_coefficient: 0.7517 - val_loss: 0.0014 - val_correlation_coefficient: 0.7523

Epoch 00150: loss did not improve from 0.00129
Epoch 151/350
9303/9303 [==============================] - 4s 404us/step - loss: 0.0013 - correlation_coefficient: 0.7529 - val_loss: 0.0014 - val_correlation_coefficient: 0.7535

Epoch 00151: loss did not improve from 0.00129
Epoch 152/350
9303/9303 [==============================] - 4s 392us/step - loss: 0.0014 - correlation_coefficient: 0.7541 - val_loss: 0.0013 - val_correlation_coefficient: 0.7546

Epoch 00152: loss did not improve from 0.00129
Epoch 153/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0012 - correlation_coefficient: 0.7553 - val_loss: 7.3983e-04 - val_correlation_coefficient: 0.7558

Epoch 00153: loss improved from 0.00129 to 0.00125, saving model to bestweights4cnn50d
Epoch 154/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0015 - correlation_coefficient: 0.7564 - val_loss: 9.2847e-04 - val_correlation_coefficient: 0.7570

Epoch 00154: loss did not improve from 0.00125
Epoch 155/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0014 - correlation_coefficient: 0.7576 - val_loss: 3.9031e-04 - val_correlation_coefficient: 0.7582

Epoch 00155: loss did not improve from 0.00125
Epoch 156/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0015 - correlation_coefficient: 0.7588 - val_loss: 6.4047e-04 - val_correlation_coefficient: 0.7593

Epoch 00156: loss did not improve from 0.00125
Epoch 157/350
9303/9303 [==============================] - 4s 403us/step - loss: 0.0016 - correlation_coefficient: 0.7600 - val_loss: 5.3265e-04 - val_correlation_coefficient: 0.7605

Epoch 00157: loss did not improve from 0.00125
Epoch 158/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0015 - correlation_coefficient: 0.7611 - val_loss: 8.2171e-04 - val_correlation_coefficient: 0.7616

Epoch 00158: loss did not improve from 0.00125
Epoch 159/350
9303/9303 [==============================] - 4s 403us/step - loss: 0.0014 - correlation_coefficient: 0.7622 - val_loss: 9.5431e-04 - val_correlation_coefficient: 0.7627

Epoch 00159: loss did not improve from 0.00125
Epoch 160/350
9303/9303 [==============================] - 4s 406us/step - loss: 0.0013 - correlation_coefficient: 0.7633 - val_loss: 3.6714e-04 - val_correlation_coefficient: 0.7638

Epoch 00160: loss did not improve from 0.00125
Epoch 161/350
9303/9303 [==============================] - 4s 401us/step - loss: 0.0014 - correlation_coefficient: 0.7645 - val_loss: 4.9587e-04 - val_correlation_coefficient: 0.7650

Epoch 00161: loss did not improve from 0.00125
Epoch 162/350
9303/9303 [==============================] - 4s 405us/step - loss: 0.0015 - correlation_coefficient: 0.7655 - val_loss: 0.0014 - val_correlation_coefficient: 0.7660

Epoch 00162: loss did not improve from 0.00125
Epoch 163/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0013 - correlation_coefficient: 0.7666 - val_loss: 0.0012 - val_correlation_coefficient: 0.7671

Epoch 00163: loss did not improve from 0.00125
Epoch 164/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0013 - correlation_coefficient: 0.7676 - val_loss: 0.0012 - val_correlation_coefficient: 0.7681

Epoch 00164: loss did not improve from 0.00125
Epoch 165/350
9303/9303 [==============================] - 4s 392us/step - loss: 0.0014 - correlation_coefficient: 0.7687 - val_loss: 5.9974e-04 - val_correlation_coefficient: 0.7692

Epoch 00165: loss did not improve from 0.00125
Epoch 166/350
9303/9303 [==============================] - 4s 399us/step - loss: 0.0013 - correlation_coefficient: 0.7697 - val_loss: 0.0011 - val_correlation_coefficient: 0.7702

Epoch 00166: loss did not improve from 0.00125
Epoch 167/350
9303/9303 [==============================] - 4s 407us/step - loss: 0.0013 - correlation_coefficient: 0.7708 - val_loss: 0.0016 - val_correlation_coefficient: 0.7712

Epoch 00167: loss did not improve from 0.00125
Epoch 168/350
9303/9303 [==============================] - 4s 385us/step - loss: 0.0012 - correlation_coefficient: 0.7717 - val_loss: 0.0011 - val_correlation_coefficient: 0.7722

Epoch 00168: loss improved from 0.00125 to 0.00117, saving model to bestweights4cnn50d
Epoch 169/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0014 - correlation_coefficient: 0.7728 - val_loss: 6.8723e-04 - val_correlation_coefficient: 0.7732

Epoch 00169: loss did not improve from 0.00117
Epoch 170/350
9303/9303 [==============================] - 4s 408us/step - loss: 0.0014 - correlation_coefficient: 0.7738 - val_loss: 7.1117e-04 - val_correlation_coefficient: 0.7742

Epoch 00170: loss did not improve from 0.00117
Epoch 171/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0013 - correlation_coefficient: 0.7748 - val_loss: 0.0012 - val_correlation_coefficient: 0.7752

Epoch 00171: loss did not improve from 0.00117
Epoch 172/350
9303/9303 [==============================] - 4s 392us/step - loss: 0.0013 - correlation_coefficient: 0.7757 - val_loss: 0.0015 - val_correlation_coefficient: 0.7762

Epoch 00172: loss did not improve from 0.00117
Epoch 173/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0014 - correlation_coefficient: 0.7767 - val_loss: 0.0011 - val_correlation_coefficient: 0.7771

Epoch 00173: loss did not improve from 0.00117
Epoch 174/350
9303/9303 [==============================] - 4s 404us/step - loss: 0.0014 - correlation_coefficient: 0.7776 - val_loss: 8.4092e-04 - val_correlation_coefficient: 0.7781

Epoch 00174: loss did not improve from 0.00117
Epoch 175/350
9303/9303 [==============================] - 4s 393us/step - loss: 0.0013 - correlation_coefficient: 0.7786 - val_loss: 8.1974e-04 - val_correlation_coefficient: 0.7790

Epoch 00175: loss did not improve from 0.00117
Epoch 176/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0013 - correlation_coefficient: 0.7795 - val_loss: 0.0010 - val_correlation_coefficient: 0.7800

Epoch 00176: loss did not improve from 0.00117
Epoch 177/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0015 - correlation_coefficient: 0.7805 - val_loss: 5.6101e-04 - val_correlation_coefficient: 0.7809

Epoch 00177: loss did not improve from 0.00117
Epoch 178/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0013 - correlation_coefficient: 0.7814 - val_loss: 4.5968e-04 - val_correlation_coefficient: 0.7819

Epoch 00178: loss did not improve from 0.00117
Epoch 179/350
9303/9303 [==============================] - 4s 394us/step - loss: 0.0014 - correlation_coefficient: 0.7824 - val_loss: 2.4822e-04 - val_correlation_coefficient: 0.7828

Epoch 00179: loss did not improve from 0.00117
Epoch 180/350
9303/9303 [==============================] - 4s 397us/step - loss: 0.0014 - correlation_coefficient: 0.7833 - val_loss: 3.7809e-04 - val_correlation_coefficient: 0.7837

Epoch 00180: loss did not improve from 0.00117
Epoch 181/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0013 - correlation_coefficient: 0.7843 - val_loss: 3.8868e-04 - val_correlation_coefficient: 0.7847

Epoch 00181: loss did not improve from 0.00117
Epoch 182/350
9303/9303 [==============================] - 4s 393us/step - loss: 0.0012 - correlation_coefficient: 0.7852 - val_loss: 8.9923e-04 - val_correlation_coefficient: 0.7856

Epoch 00182: loss improved from 0.00117 to 0.00115, saving model to bestweights4cnn50d
Epoch 183/350
9303/9303 [==============================] - 4s 397us/step - loss: 0.0012 - correlation_coefficient: 0.7861 - val_loss: 1.8527e-04 - val_correlation_coefficient: 0.7865

Epoch 00183: loss did not improve from 0.00115
Epoch 184/350
9303/9303 [==============================] - 4s 418us/step - loss: 0.0013 - correlation_coefficient: 0.7870 - val_loss: 7.6237e-04 - val_correlation_coefficient: 0.7874

Epoch 00184: loss did not improve from 0.00115
Epoch 185/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0012 - correlation_coefficient: 0.7879 - val_loss: 5.5162e-04 - val_correlation_coefficient: 0.7883

Epoch 00185: loss did not improve from 0.00115
Epoch 186/350
9303/9303 [==============================] - 4s 395us/step - loss: 0.0013 - correlation_coefficient: 0.7888 - val_loss: 5.3818e-04 - val_correlation_coefficient: 0.7892

Epoch 00186: loss did not improve from 0.00115
Epoch 187/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0014 - correlation_coefficient: 0.7896 - val_loss: 2.4351e-04 - val_correlation_coefficient: 0.7900

Epoch 00187: loss did not improve from 0.00115
Epoch 188/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0013 - correlation_coefficient: 0.7905 - val_loss: 9.3374e-04 - val_correlation_coefficient: 0.7909

Epoch 00188: loss did not improve from 0.00115
Epoch 189/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0011 - correlation_coefficient: 0.7914 - val_loss: 0.0014 - val_correlation_coefficient: 0.7917

Epoch 00189: loss improved from 0.00115 to 0.00114, saving model to bestweights4cnn50d
Epoch 190/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0012 - correlation_coefficient: 0.7922 - val_loss: 6.1499e-04 - val_correlation_coefficient: 0.7926

Epoch 00190: loss did not improve from 0.00114
Epoch 191/350
9303/9303 [==============================] - 4s 385us/step - loss: 0.0013 - correlation_coefficient: 0.7930 - val_loss: 6.4920e-04 - val_correlation_coefficient: 0.7934

Epoch 00191: loss did not improve from 0.00114
Epoch 192/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0013 - correlation_coefficient: 0.7938 - val_loss: 3.5601e-04 - val_correlation_coefficient: 0.7942

Epoch 00192: loss did not improve from 0.00114
Epoch 193/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0014 - correlation_coefficient: 0.7947 - val_loss: 4.8247e-04 - val_correlation_coefficient: 0.7950

Epoch 00193: loss did not improve from 0.00114
Epoch 194/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0013 - correlation_coefficient: 0.7955 - val_loss: 7.7421e-04 - val_correlation_coefficient: 0.7959

Epoch 00194: loss did not improve from 0.00114
Epoch 195/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0012 - correlation_coefficient: 0.7963 - val_loss: 7.0618e-04 - val_correlation_coefficient: 0.7967

Epoch 00195: loss did not improve from 0.00114
Epoch 196/350
9303/9303 [==============================] - 4s 385us/step - loss: 0.0012 - correlation_coefficient: 0.7971 - val_loss: 7.3518e-04 - val_correlation_coefficient: 0.7975

Epoch 00196: loss did not improve from 0.00114
Epoch 197/350
9303/9303 [==============================] - 4s 393us/step - loss: 0.0012 - correlation_coefficient: 0.7979 - val_loss: 8.4643e-04 - val_correlation_coefficient: 0.7982

Epoch 00197: loss did not improve from 0.00114
Epoch 198/350
9303/9303 [==============================] - 4s 376us/step - loss: 0.0011 - correlation_coefficient: 0.7987 - val_loss: 7.1018e-04 - val_correlation_coefficient: 0.7990

Epoch 00198: loss did not improve from 0.00114
Epoch 199/350
9303/9303 [==============================] - 4s 393us/step - loss: 0.0013 - correlation_coefficient: 0.7994 - val_loss: 4.5874e-04 - val_correlation_coefficient: 0.7998

Epoch 00199: loss did not improve from 0.00114
Epoch 200/350
9303/9303 [==============================] - 4s 410us/step - loss: 0.0013 - correlation_coefficient: 0.8002 - val_loss: 5.3620e-04 - val_correlation_coefficient: 0.8006

Epoch 00200: loss did not improve from 0.00114
Epoch 201/350
9303/9303 [==============================] - 4s 394us/step - loss: 0.0014 - correlation_coefficient: 0.8010 - val_loss: 0.0010 - val_correlation_coefficient: 0.8013

Epoch 00201: loss did not improve from 0.00114
Epoch 202/350
9303/9303 [==============================] - 4s 378us/step - loss: 0.0012 - correlation_coefficient: 0.8017 - val_loss: 4.7496e-04 - val_correlation_coefficient: 0.8021

Epoch 00202: loss did not improve from 0.00114
Epoch 203/350
9303/9303 [==============================] - 4s 392us/step - loss: 0.0014 - correlation_coefficient: 0.8025 - val_loss: 5.8045e-04 - val_correlation_coefficient: 0.8028

Epoch 00203: loss did not improve from 0.00114
Epoch 204/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0012 - correlation_coefficient: 0.8032 - val_loss: 7.9651e-04 - val_correlation_coefficient: 0.8036

Epoch 00204: loss did not improve from 0.00114
Epoch 205/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0013 - correlation_coefficient: 0.8040 - val_loss: 7.9223e-04 - val_correlation_coefficient: 0.8043

Epoch 00205: loss did not improve from 0.00114
Epoch 206/350
9303/9303 [==============================] - 4s 393us/step - loss: 0.0012 - correlation_coefficient: 0.8047 - val_loss: 9.3465e-04 - val_correlation_coefficient: 0.8050

Epoch 00206: loss did not improve from 0.00114
Epoch 207/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0013 - correlation_coefficient: 0.8054 - val_loss: 6.8121e-04 - val_correlation_coefficient: 0.8057

Epoch 00207: loss did not improve from 0.00114
Epoch 208/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0013 - correlation_coefficient: 0.8061 - val_loss: 6.7980e-04 - val_correlation_coefficient: 0.8065

Epoch 00208: loss did not improve from 0.00114
Epoch 209/350
9303/9303 [==============================] - 4s 401us/step - loss: 0.0013 - correlation_coefficient: 0.8068 - val_loss: 4.0947e-04 - val_correlation_coefficient: 0.8072

Epoch 00209: loss did not improve from 0.00114
Epoch 210/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0013 - correlation_coefficient: 0.8076 - val_loss: 6.2694e-04 - val_correlation_coefficient: 0.8079

Epoch 00210: loss did not improve from 0.00114
Epoch 211/350
9303/9303 [==============================] - 4s 377us/step - loss: 0.0012 - correlation_coefficient: 0.8083 - val_loss: 7.6893e-04 - val_correlation_coefficient: 0.8086

Epoch 00211: loss did not improve from 0.00114
Epoch 212/350
9303/9303 [==============================] - 4s 392us/step - loss: 0.0011 - correlation_coefficient: 0.8090 - val_loss: 7.3236e-04 - val_correlation_coefficient: 0.8093

Epoch 00212: loss improved from 0.00114 to 0.00113, saving model to bestweights4cnn50d
Epoch 213/350
9303/9303 [==============================] - 4s 408us/step - loss: 0.0012 - correlation_coefficient: 0.8097 - val_loss: 3.7753e-04 - val_correlation_coefficient: 0.8100

Epoch 00213: loss did not improve from 0.00113
Epoch 214/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0015 - correlation_coefficient: 0.8104 - val_loss: 0.0012 - val_correlation_coefficient: 0.8107

Epoch 00214: loss did not improve from 0.00113
Epoch 215/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0012 - correlation_coefficient: 0.8110 - val_loss: 3.8552e-04 - val_correlation_coefficient: 0.8113

Epoch 00215: loss did not improve from 0.00113
Epoch 216/350
9303/9303 [==============================] - 4s 396us/step - loss: 0.0011 - correlation_coefficient: 0.8117 - val_loss: 8.3789e-04 - val_correlation_coefficient: 0.8120

Epoch 00216: loss improved from 0.00113 to 0.00108, saving model to bestweights4cnn50d
Epoch 217/350
9303/9303 [==============================] - 4s 404us/step - loss: 0.0012 - correlation_coefficient: 0.8124 - val_loss: 4.6598e-04 - val_correlation_coefficient: 0.8127

Epoch 00217: loss did not improve from 0.00108
Epoch 218/350
9303/9303 [==============================] - 4s 386us/step - loss: 0.0011 - correlation_coefficient: 0.8131 - val_loss: 8.6074e-04 - val_correlation_coefficient: 0.8134

Epoch 00218: loss did not improve from 0.00108
Epoch 219/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0012 - correlation_coefficient: 0.8137 - val_loss: 6.6244e-04 - val_correlation_coefficient: 0.8140

Epoch 00219: loss did not improve from 0.00108
Epoch 220/350
9303/9303 [==============================] - 4s 443us/step - loss: 0.0012 - correlation_coefficient: 0.8144 - val_loss: 4.9309e-04 - val_correlation_coefficient: 0.8147

Epoch 00220: loss did not improve from 0.00108
Epoch 221/350
9303/9303 [==============================] - 4s 399us/step - loss: 0.0012 - correlation_coefficient: 0.8150 - val_loss: 6.0113e-04 - val_correlation_coefficient: 0.8153

Epoch 00221: loss did not improve from 0.00108
Epoch 222/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0013 - correlation_coefficient: 0.8157 - val_loss: 7.5115e-04 - val_correlation_coefficient: 0.8160

Epoch 00222: loss did not improve from 0.00108
Epoch 223/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0011 - correlation_coefficient: 0.8163 - val_loss: 3.2262e-04 - val_correlation_coefficient: 0.8166

Epoch 00223: loss did not improve from 0.00108
Epoch 224/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0012 - correlation_coefficient: 0.8170 - val_loss: 4.7678e-04 - val_correlation_coefficient: 0.8173

Epoch 00224: loss did not improve from 0.00108
Epoch 225/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0011 - correlation_coefficient: 0.8176 - val_loss: 7.0648e-04 - val_correlation_coefficient: 0.8179

Epoch 00225: loss did not improve from 0.00108
Epoch 226/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0011 - correlation_coefficient: 0.8182 - val_loss: 4.1860e-04 - val_correlation_coefficient: 0.8185

Epoch 00226: loss did not improve from 0.00108
Epoch 227/350
9303/9303 [==============================] - 4s 398us/step - loss: 0.0012 - correlation_coefficient: 0.8189 - val_loss: 9.4022e-04 - val_correlation_coefficient: 0.8192

Epoch 00227: loss did not improve from 0.00108
Epoch 228/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0012 - correlation_coefficient: 0.8195 - val_loss: 6.2315e-04 - val_correlation_coefficient: 0.8198

Epoch 00228: loss did not improve from 0.00108
Epoch 229/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0010 - correlation_coefficient: 0.8201 - val_loss: 4.7836e-04 - val_correlation_coefficient: 0.8204

Epoch 00229: loss improved from 0.00108 to 0.00103, saving model to bestweights4cnn50d
Epoch 230/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0012 - correlation_coefficient: 0.8207 - val_loss: 6.9825e-04 - val_correlation_coefficient: 0.8210

Epoch 00230: loss did not improve from 0.00103
Epoch 231/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0012 - correlation_coefficient: 0.8213 - val_loss: 3.4683e-04 - val_correlation_coefficient: 0.8216

Epoch 00231: loss did not improve from 0.00103
Epoch 232/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0011 - correlation_coefficient: 0.8219 - val_loss: 7.6293e-04 - val_correlation_coefficient: 0.8222

Epoch 00232: loss did not improve from 0.00103
Epoch 233/350
9303/9303 [==============================] - 4s 404us/step - loss: 0.0013 - correlation_coefficient: 0.8225 - val_loss: 6.5537e-04 - val_correlation_coefficient: 0.8228

Epoch 00233: loss did not improve from 0.00103
Epoch 234/350
9303/9303 [==============================] - 4s 394us/step - loss: 0.0011 - correlation_coefficient: 0.8231 - val_loss: 5.1882e-04 - val_correlation_coefficient: 0.8234

Epoch 00234: loss did not improve from 0.00103
Epoch 235/350
9303/9303 [==============================] - 4s 401us/step - loss: 0.0011 - correlation_coefficient: 0.8237 - val_loss: 5.5275e-04 - val_correlation_coefficient: 0.8240

Epoch 00235: loss did not improve from 0.00103
Epoch 236/350
9303/9303 [==============================] - 4s 386us/step - loss: 0.0011 - correlation_coefficient: 0.8243 - val_loss: 0.0011 - val_correlation_coefficient: 0.8246

Epoch 00236: loss did not improve from 0.00103
Epoch 237/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0012 - correlation_coefficient: 0.8249 - val_loss: 6.9870e-04 - val_correlation_coefficient: 0.8252

Epoch 00237: loss did not improve from 0.00103
Epoch 238/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0012 - correlation_coefficient: 0.8255 - val_loss: 5.2619e-04 - val_correlation_coefficient: 0.8257

Epoch 00238: loss did not improve from 0.00103
Epoch 239/350
9303/9303 [==============================] - 4s 393us/step - loss: 0.0014 - correlation_coefficient: 0.8260 - val_loss: 0.0013 - val_correlation_coefficient: 0.8263

Epoch 00239: loss did not improve from 0.00103
Epoch 240/350
9303/9303 [==============================] - 4s 378us/step - loss: 0.0012 - correlation_coefficient: 0.8266 - val_loss: 7.3460e-04 - val_correlation_coefficient: 0.8268

Epoch 00240: loss did not improve from 0.00103
Epoch 241/350
9303/9303 [==============================] - 4s 386us/step - loss: 0.0011 - correlation_coefficient: 0.8271 - val_loss: 0.0012 - val_correlation_coefficient: 0.8274

Epoch 00241: loss did not improve from 0.00103
Epoch 242/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0012 - correlation_coefficient: 0.8277 - val_loss: 0.0012 - val_correlation_coefficient: 0.8279

Epoch 00242: loss did not improve from 0.00103
Epoch 243/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0012 - correlation_coefficient: 0.8282 - val_loss: 5.9293e-04 - val_correlation_coefficient: 0.8285

Epoch 00243: loss did not improve from 0.00103
Epoch 244/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0011 - correlation_coefficient: 0.8288 - val_loss: 5.7638e-04 - val_correlation_coefficient: 0.8290

Epoch 00244: loss did not improve from 0.00103
Epoch 245/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0013 - correlation_coefficient: 0.8293 - val_loss: 0.0011 - val_correlation_coefficient: 0.8296

Epoch 00245: loss did not improve from 0.00103
Epoch 246/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0010 - correlation_coefficient: 0.8299 - val_loss: 8.7166e-04 - val_correlation_coefficient: 0.8301

Epoch 00246: loss improved from 0.00103 to 0.00103, saving model to bestweights4cnn50d
Epoch 247/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0012 - correlation_coefficient: 0.8304 - val_loss: 5.8546e-04 - val_correlation_coefficient: 0.8307

Epoch 00247: loss did not improve from 0.00103
Epoch 248/350
9303/9303 [==============================] - 4s 378us/step - loss: 0.0011 - correlation_coefficient: 0.8309 - val_loss: 4.1412e-04 - val_correlation_coefficient: 0.8312

Epoch 00248: loss did not improve from 0.00103
Epoch 249/350
9303/9303 [==============================] - 4s 379us/step - loss: 0.0011 - correlation_coefficient: 0.8315 - val_loss: 0.0013 - val_correlation_coefficient: 0.8317

Epoch 00249: loss did not improve from 0.00103
Epoch 250/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0012 - correlation_coefficient: 0.8320 - val_loss: 7.2710e-04 - val_correlation_coefficient: 0.8322

Epoch 00250: loss did not improve from 0.00103
Epoch 251/350
9303/9303 [==============================] - 4s 377us/step - loss: 9.9566e-04 - correlation_coefficient: 0.8325 - val_loss: 2.7470e-04 - val_correlation_coefficient: 0.8328

Epoch 00251: loss improved from 0.00103 to 0.00100, saving model to bestweights4cnn50d
Epoch 252/350
9303/9303 [==============================] - 4s 411us/step - loss: 0.0011 - correlation_coefficient: 0.8331 - val_loss: 2.8242e-04 - val_correlation_coefficient: 0.8333

Epoch 00252: loss did not improve from 0.00100
Epoch 253/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0013 - correlation_coefficient: 0.8336 - val_loss: 7.4361e-04 - val_correlation_coefficient: 0.8338

Epoch 00253: loss did not improve from 0.00100
Epoch 254/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0011 - correlation_coefficient: 0.8341 - val_loss: 9.2916e-04 - val_correlation_coefficient: 0.8343

Epoch 00254: loss did not improve from 0.00100
Epoch 255/350
9303/9303 [==============================] - 3s 376us/step - loss: 0.0011 - correlation_coefficient: 0.8346 - val_loss: 7.2641e-04 - val_correlation_coefficient: 0.8349

Epoch 00255: loss did not improve from 0.00100
Epoch 256/350
9303/9303 [==============================] - 4s 399us/step - loss: 0.0012 - correlation_coefficient: 0.8351 - val_loss: 3.1095e-04 - val_correlation_coefficient: 0.8354

Epoch 00256: loss did not improve from 0.00100
Epoch 257/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0013 - correlation_coefficient: 0.8356 - val_loss: 6.0639e-04 - val_correlation_coefficient: 0.8359

Epoch 00257: loss did not improve from 0.00100
Epoch 258/350
9303/9303 [==============================] - 4s 399us/step - loss: 0.0012 - correlation_coefficient: 0.8361 - val_loss: 5.0356e-04 - val_correlation_coefficient: 0.8364

Epoch 00258: loss did not improve from 0.00100
Epoch 259/350
9303/9303 [==============================] - 4s 396us/step - loss: 0.0013 - correlation_coefficient: 0.8366 - val_loss: 0.0017 - val_correlation_coefficient: 0.8369

Epoch 00259: loss did not improve from 0.00100
Epoch 260/350
9303/9303 [==============================] - 4s 394us/step - loss: 0.0012 - correlation_coefficient: 0.8371 - val_loss: 5.2799e-04 - val_correlation_coefficient: 0.8373

Epoch 00260: loss did not improve from 0.00100
Epoch 261/350
9303/9303 [==============================] - 4s 393us/step - loss: 0.0012 - correlation_coefficient: 0.8376 - val_loss: 8.8189e-04 - val_correlation_coefficient: 0.8378

Epoch 00261: loss did not improve from 0.00100
Epoch 262/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0013 - correlation_coefficient: 0.8381 - val_loss: 5.8735e-04 - val_correlation_coefficient: 0.8383

Epoch 00262: loss did not improve from 0.00100
Epoch 263/350
9303/9303 [==============================] - 4s 399us/step - loss: 0.0011 - correlation_coefficient: 0.8386 - val_loss: 2.2366e-04 - val_correlation_coefficient: 0.8388

Epoch 00263: loss did not improve from 0.00100
Epoch 264/350
9303/9303 [==============================] - 4s 395us/step - loss: 0.0012 - correlation_coefficient: 0.8391 - val_loss: 0.0014 - val_correlation_coefficient: 0.8393

Epoch 00264: loss did not improve from 0.00100
Epoch 265/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0011 - correlation_coefficient: 0.8395 - val_loss: 0.0014 - val_correlation_coefficient: 0.8397

Epoch 00265: loss did not improve from 0.00100
Epoch 266/350
9303/9303 [==============================] - 4s 407us/step - loss: 0.0011 - correlation_coefficient: 0.8400 - val_loss: 0.0013 - val_correlation_coefficient: 0.8402

Epoch 00266: loss did not improve from 0.00100
Epoch 267/350
9303/9303 [==============================] - 4s 396us/step - loss: 0.0011 - correlation_coefficient: 0.8404 - val_loss: 4.9267e-04 - val_correlation_coefficient: 0.8407

Epoch 00267: loss did not improve from 0.00100
Epoch 268/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0012 - correlation_coefficient: 0.8409 - val_loss: 8.8473e-04 - val_correlation_coefficient: 0.8411

Epoch 00268: loss did not improve from 0.00100
Epoch 269/350
9303/9303 [==============================] - 4s 386us/step - loss: 0.0011 - correlation_coefficient: 0.8414 - val_loss: 4.1079e-04 - val_correlation_coefficient: 0.8416

Epoch 00269: loss did not improve from 0.00100
Epoch 270/350
9303/9303 [==============================] - 4s 384us/step - loss: 9.9639e-04 - correlation_coefficient: 0.8419 - val_loss: 2.9554e-04 - val_correlation_coefficient: 0.8421

Epoch 00270: loss did not improve from 0.00100
Epoch 271/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0011 - correlation_coefficient: 0.8423 - val_loss: 2.3577e-04 - val_correlation_coefficient: 0.8425

Epoch 00271: loss did not improve from 0.00100
Epoch 272/350
9303/9303 [==============================] - 4s 384us/step - loss: 0.0011 - correlation_coefficient: 0.8428 - val_loss: 6.5750e-04 - val_correlation_coefficient: 0.8430

Epoch 00272: loss did not improve from 0.00100
Epoch 273/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0011 - correlation_coefficient: 0.8433 - val_loss: 7.6905e-04 - val_correlation_coefficient: 0.8435

Epoch 00273: loss did not improve from 0.00100
Epoch 274/350
9303/9303 [==============================] - 4s 403us/step - loss: 0.0011 - correlation_coefficient: 0.8437 - val_loss: 3.1529e-04 - val_correlation_coefficient: 0.8439

Epoch 00274: loss did not improve from 0.00100
Epoch 275/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0012 - correlation_coefficient: 0.8442 - val_loss: 5.2504e-04 - val_correlation_coefficient: 0.8444

Epoch 00275: loss did not improve from 0.00100
Epoch 276/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0011 - correlation_coefficient: 0.8446 - val_loss: 2.6126e-04 - val_correlation_coefficient: 0.8448

Epoch 00276: loss did not improve from 0.00100
Epoch 277/350
9303/9303 [==============================] - 4s 402us/step - loss: 0.0012 - correlation_coefficient: 0.8451 - val_loss: 6.2932e-04 - val_correlation_coefficient: 0.8453

Epoch 00277: loss did not improve from 0.00100
Epoch 278/350
9303/9303 [==============================] - 4s 389us/step - loss: 0.0010 - correlation_coefficient: 0.8455 - val_loss: 2.9038e-04 - val_correlation_coefficient: 0.8457

Epoch 00278: loss did not improve from 0.00100
Epoch 279/350
9303/9303 [==============================] - 4s 390us/step - loss: 0.0011 - correlation_coefficient: 0.8460 - val_loss: 1.5426e-04 - val_correlation_coefficient: 0.8462

Epoch 00279: loss did not improve from 0.00100
Epoch 280/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0012 - correlation_coefficient: 0.8464 - val_loss: 7.3100e-04 - val_correlation_coefficient: 0.8466

Epoch 00280: loss did not improve from 0.00100
Epoch 281/350
9303/9303 [==============================] - 4s 393us/step - loss: 0.0012 - correlation_coefficient: 0.8468 - val_loss: 0.0010 - val_correlation_coefficient: 0.8470

Epoch 00281: loss did not improve from 0.00100
Epoch 282/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0011 - correlation_coefficient: 0.8473 - val_loss: 3.4540e-04 - val_correlation_coefficient: 0.8475

Epoch 00282: loss did not improve from 0.00100
Epoch 283/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0010 - correlation_coefficient: 0.8477 - val_loss: 6.9031e-04 - val_correlation_coefficient: 0.8479

Epoch 00283: loss did not improve from 0.00100
Epoch 284/350
9303/9303 [==============================] - 4s 379us/step - loss: 0.0011 - correlation_coefficient: 0.8481 - val_loss: 4.1144e-04 - val_correlation_coefficient: 0.8483

Epoch 00284: loss did not improve from 0.00100
Epoch 285/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0012 - correlation_coefficient: 0.8486 - val_loss: 0.0011 - val_correlation_coefficient: 0.8487

Epoch 00285: loss did not improve from 0.00100
Epoch 286/350
9303/9303 [==============================] - 4s 382us/step - loss: 0.0012 - correlation_coefficient: 0.8490 - val_loss: 4.0766e-04 - val_correlation_coefficient: 0.8492

Epoch 00286: loss did not improve from 0.00100
Epoch 287/350
9303/9303 [==============================] - 4s 388us/step - loss: 0.0010 - correlation_coefficient: 0.8494 - val_loss: 4.8526e-04 - val_correlation_coefficient: 0.8496

Epoch 00287: loss did not improve from 0.00100
Epoch 288/350
9303/9303 [==============================] - 4s 383us/step - loss: 0.0012 - correlation_coefficient: 0.8498 - val_loss: 2.7656e-04 - val_correlation_coefficient: 0.8500

Epoch 00288: loss did not improve from 0.00100
Epoch 289/350
9303/9303 [==============================] - 4s 385us/step - loss: 0.0012 - correlation_coefficient: 0.8502 - val_loss: 2.2220e-04 - val_correlation_coefficient: 0.8504

Epoch 00289: loss did not improve from 0.00100
Epoch 290/350
9303/9303 [==============================] - 4s 379us/step - loss: 0.0011 - correlation_coefficient: 0.8507 - val_loss: 3.3932e-04 - val_correlation_coefficient: 0.8508

Epoch 00290: loss did not improve from 0.00100
Epoch 291/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0011 - correlation_coefficient: 0.8511 - val_loss: 4.9258e-04 - val_correlation_coefficient: 0.8513

Epoch 00291: loss did not improve from 0.00100
Epoch 292/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0012 - correlation_coefficient: 0.8515 - val_loss: 5.1395e-04 - val_correlation_coefficient: 0.8517

Epoch 00292: loss did not improve from 0.00100
Epoch 293/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0012 - correlation_coefficient: 0.8519 - val_loss: 0.0014 - val_correlation_coefficient: 0.8521

Epoch 00293: loss did not improve from 0.00100
Epoch 294/350
9303/9303 [==============================] - 4s 377us/step - loss: 0.0013 - correlation_coefficient: 0.8523 - val_loss: 6.6831e-04 - val_correlation_coefficient: 0.8524

Epoch 00294: loss did not improve from 0.00100
Epoch 295/350
9303/9303 [==============================] - 4s 385us/step - loss: 0.0010 - correlation_coefficient: 0.8526 - val_loss: 5.0352e-04 - val_correlation_coefficient: 0.8528

Epoch 00295: loss did not improve from 0.00100
Epoch 296/350
9303/9303 [==============================] - 4s 381us/step - loss: 0.0011 - correlation_coefficient: 0.8530 - val_loss: 0.0012 - val_correlation_coefficient: 0.8532

Epoch 00296: loss did not improve from 0.00100
Epoch 297/350
9303/9303 [==============================] - 4s 401us/step - loss: 0.0012 - correlation_coefficient: 0.8534 - val_loss: 8.1394e-04 - val_correlation_coefficient: 0.8536

Epoch 00297: loss did not improve from 0.00100
Epoch 298/350
9303/9303 [==============================] - 4s 398us/step - loss: 0.0012 - correlation_coefficient: 0.8538 - val_loss: 8.2065e-04 - val_correlation_coefficient: 0.8540

Epoch 00298: loss did not improve from 0.00100
Epoch 299/350
9303/9303 [==============================] - 4s 387us/step - loss: 0.0011 - correlation_coefficient: 0.8542 - val_loss: 7.2024e-04 - val_correlation_coefficient: 0.8544

Epoch 00299: loss did not improve from 0.00100
Epoch 300/350
9303/9303 [==============================] - 4s 391us/step - loss: 0.0013 - correlation_coefficient: 0.8546 - val_loss: 5.1049e-04 - val_correlation_coefficient: 0.8547

Epoch 00300: loss did not improve from 0.00100
Epoch 301/350
9303/9303 [==============================] - 4s 380us/step - loss: 0.0011 - correlation_coefficient: 0.8549 - val_loss: 5.7252e-04 - val_correlation_coefficient: 0.8551

Epoch 00301: loss did not improve from 0.00100
Epoch 00301: early stopping
train Corelation: 85.535640
train loss: 0.027683

epoch with minium val_loss is:  251
 loss minimum value is:  0.0009956601064115215
avg loss is:  [1.0449456880006254, 0.4231792419370395, 0.19814640380971524, 0.1189084587089928, 0.08175408217095309, 0.05758576470716633, 0.04231137961852388, 0.034667186175548016, 0.02733631842827446, 0.022319212072154187, 0.01853066969700546, 0.01584359360345234, 0.013986329895742642, 0.011891670514061915, 0.010491158070923096, 0.009650299712610312, 0.008519137939224989, 0.007493194726233801, 0.007225031924987514, 0.006471596845635596, 0.006268603855161965, 0.0056813005597243435, 0.005632669732143187, 0.0050545985697134236, 0.004979614443157205, 0.004686021575477768, 0.004769454934531856, 0.004435378437541248, 0.004153141861055281, 0.003971684373557603, 0.003991828144237753, 0.004019733542538591, 0.004146505998728646, 0.0035387338286778894, 0.004164108755636116, 0.003638553519014561, 0.0037569419916989094, 0.00339833346853141, 0.00373549898680911, 0.003759250037414559, 0.0034629385052428034, 0.003188555424296765, 0.0035567367161586792, 0.0032587774218251586, 0.0032248908950080575, 0.003322028841490574, 0.003356244927207129, 0.0032228047357195184, 0.0029938881448968104, 0.0030542751654059953, 0.0035473067889765495, 0.003369943587465137, 0.00318155474072829, 0.0029311064690512464, 0.0031402980840977072, 0.0031577347418564453, 0.002909198574421283, 0.002822221062479193, 0.0026679469383015667, 0.0027036943616980023, 0.0027279240200807605, 0.0028138261482217267, 0.0029566522794065384, 0.00282629304451364, 0.0027947865209201742, 0.002817455236342012, 0.002693001177158128, 0.0026992478523145923, 0.0029151458677371352, 0.002722838245396111, 0.0027376586329270008, 0.0029373264242947505, 0.002726332400662669, 0.0026038054385450044, 0.0026546282566654164, 0.002565810888796231, 0.0025094683426435814, 0.003001921782979853, 0.002702379832044244, 0.0025049720073647788, 0.0023228841146941148, 0.002226685762831263, 0.002436767537806531, 0.002393854021400175, 0.002072354150507737, 0.002311586997165964, 0.0024291487872746915, 0.0025196307189785617, 0.0022851600074486078, 0.0023571337340174375, 0.0022720344583933875, 0.0021723493656423255, 0.0020136694648050513, 0.002109346117514576, 0.0020503854426309766, 0.002224458783024452, 0.0021886709747611947, 0.0024689210208242384, 0.002189018341988857, 0.002062643021120892, 0.002356832175561943, 0.002006476195656243, 0.0020566396703988158, 0.0020255988601560727, 0.00207217246378164, 0.0017773808698108519, 0.0018716067989282435, 0.0019689154660650675, 0.001777565264072272, 0.0019433301119727482, 0.0018473907568584396, 0.001808841464825605, 0.0017712105678857545, 0.0018252806284420947, 0.0019093330054662343, 0.0017786464316113314, 0.0018840249289444558, 0.0016080901031556073, 0.0017513607132698976, 0.0018949886517489905, 0.001618615434458294, 0.0016212457252679769, 0.001556216039775234, 0.0014108553807892158, 0.0015247016308738489, 0.0016632143649925638, 0.0017577253831698642, 0.0018031489499141338, 0.001775326015644747, 0.0016623461063109129, 0.0016034014361752248, 0.0016701544739700306, 0.0016148646354267244, 0.0016817296184915747, 0.001543372072109817, 0.0013942283266287036, 0.0016233504576744489, 0.001559979266051684, 0.0016591277395924776, 0.001602632260584939, 0.0017014221422611308, 0.0016118246973320517, 0.0015689682648027154, 0.0013277393568130568, 0.0014438984005014307, 0.0014372536136508373, 0.0014544336311609245, 0.001419711968711906, 0.0012949050631531093, 0.0012981464435025774, 0.0013489906818544993, 0.0014295019634476662, 0.0012495526924807833, 0.0014705354019014022, 0.001373962794050036, 0.0014683374079455087, 0.0016356154474518184, 0.001523405827756474, 0.0013714793595107674, 0.0013433920030733288, 0.0014209779863691575, 0.0014742537901465398, 0.0012829519944529476, 0.001323310678288844, 0.001369156203404194, 0.0013334327277519297, 0.001277928308571883, 0.0011740055641404786, 0.0013800589877557069, 0.0013504697514575175, 0.0013254919310945203, 0.0012979053753768234, 0.0013918419969462356, 0.0013586518528254461, 0.0013106208709885994, 0.0013253598429141906, 0.0015262213160970517, 0.0012683551144353404, 0.001356490927257443, 0.0013692629111798182, 0.0013103226711895964, 0.0011502396436519284, 0.0012448058966885622, 0.00125952885251628, 0.0011990694388207422, 0.0012594936233478178, 0.001399997303124802, 0.0013225626431478724, 0.0011412366582563252, 0.0012147434807230257, 0.0012925784492999952, 0.0012977053279159857, 0.0013984507116340248, 0.0012778445946993804, 0.0012388794547671478, 0.0012341785354964818, 0.0012430139215837065, 0.0011494933369712737, 0.0013088065423399744, 0.0013214937830346345, 0.0014137527090760103, 0.0011806058941845872, 0.0013963644098515303, 0.0011554643617751018, 0.0012863515181971291, 0.0012115195032065401, 0.001277310780706688, 0.0012628040625788847, 0.0012660216647015583, 0.0012730372061336872, 0.0011507614288372172, 0.0011262383963852659, 0.0012272245912281266, 0.0014619579689418448, 0.001215290920163703, 0.0010801407283154592, 0.0012088881369910819, 0.0011113463061261334, 0.001169342863376762, 0.0011760561248924958, 0.001197588848660681, 0.0012996888866135116, 0.001139527683168297, 0.0012396866143131004, 0.0011164593598126122, 0.0011480869417633702, 0.001224152668233387, 0.001202379644382745, 0.001028963422424163, 0.0012478341076070168, 0.001165988499698731, 0.0011394959930512716, 0.00125958031186142, 0.0011234956424682131, 0.0011012840615777157, 0.0011177863772026694, 0.0011727789446792029, 0.001157712949638401, 0.001386860011000914, 0.0012044940542273615, 0.0011477569741026043, 0.0011607353474491731, 0.0011877168461381683, 0.00105050818571777, 0.001274140959412087, 0.0010251886035943082, 0.0011841052871958398, 0.0010980060304551403, 0.0010865496128309895, 0.0011533759670633605, 0.0009956601064115215, 0.00112872440114659, 0.0012588227588481295, 0.0011232129787587931, 0.0010699950451092193, 0.0012166973968861078, 0.0012735742463937604, 0.0011736091196706804, 0.0012783532908694274, 0.0012046750150626945, 0.0012364914977604086, 0.0013238804667120322, 0.0010638508749012544, 0.0011926315415810382, 0.0010771535681324375, 0.0010774048103791842, 0.001144382363894265, 0.0012137990717798245, 0.001063804337028548, 0.0009963887325292516, 0.001148744686091733, 0.0010770497009507318, 0.0010723892804281772, 0.0011365894963289579, 0.0011625798691617126, 0.001140227284345315, 0.001150796679241496, 0.0010348407069466809, 0.0011068465456615412, 0.001240958093373119, 0.0011804415685080947, 0.001138679790025749, 0.0010029266337401794, 0.0011267583862743535, 0.0011772050821070898, 0.0011853952036184848, 0.0010408000360025483, 0.0012023099862556858, 0.001168804002642843, 0.0010942903852718328, 0.001104604007784763, 0.0011623575587471969, 0.0012298902784336342, 0.001295423438869866, 0.0010441653319089598, 0.0010747928810300388, 0.0011944230234818651, 0.001206223025824304, 0.0011470279856933348, 0.001256254114303007, 0.0011041080651240786]

training correlation value is:  0.8325329
correlation
[0.07126381, 0.08131133, 0.091838315, 0.095839314, 0.09693407, 0.10032385, 0.105357945, 0.11295621, 0.12300797, 0.13533941, 0.14957792, 0.16305931, 0.1770331, 0.1904229, 0.20443423, 0.21761532, 0.23085931, 0.24417736, 0.25697076, 0.2689818, 0.28116465, 0.29252324, 0.3036097, 0.31436637, 0.3249826, 0.3350434, 0.34524953, 0.35470942, 0.3640577, 0.3726046, 0.38121912, 0.38939956, 0.39688078, 0.4050131, 0.4122644, 0.41981676, 0.4271793, 0.43416536, 0.4410746, 0.4480762, 0.45463455, 0.46099266, 0.46713734, 0.47276986, 0.47859305, 0.48411033, 0.48967558, 0.4951206, 0.5003828, 0.5056651, 0.510657, 0.51562023, 0.52056485, 0.5253501, 0.5301277, 0.5344774, 0.5386492, 0.54313785, 0.54771346, 0.5517847, 0.5559265, 0.5598471, 0.56370825, 0.5672149, 0.57124543, 0.57500345, 0.5786801, 0.5821558, 0.58592224, 0.58935136, 0.5928631, 0.5961726, 0.59954584, 0.6028239, 0.60589975, 0.60895485, 0.61202526, 0.61496186, 0.6181781, 0.6211419, 0.62406033, 0.626893, 0.6296614, 0.6322709, 0.635126, 0.63786054, 0.64039475, 0.6430253, 0.64549905, 0.64803356, 0.6506098, 0.6530574, 0.65547156, 0.6578924, 0.6601957, 0.66245013, 0.66473037, 0.6669283, 0.6690688, 0.6712673, 0.67347354, 0.6756732, 0.6777302, 0.67978907, 0.6818335, 0.68387276, 0.685905, 0.6879634, 0.6900074, 0.6918917, 0.6938304, 0.69579166, 0.6976554, 0.6993919, 0.70118135, 0.7028615, 0.70450884, 0.7062708, 0.7078992, 0.709599, 0.71125734, 0.7128069, 0.71442527, 0.7159848, 0.7176313, 0.7191125, 0.720601, 0.72209054, 0.72363925, 0.72509944, 0.72660154, 0.7280858, 0.7294472, 0.73093224, 0.7324114, 0.7338218, 0.73522073, 0.7365491, 0.7379848, 0.7393293, 0.74056405, 0.741826, 0.74312204, 0.74442005, 0.7456954, 0.7469377, 0.7481257, 0.7493601, 0.7505707, 0.75174, 0.75293684, 0.7541049, 0.75526714, 0.75644964, 0.75761354, 0.7588059, 0.75995636, 0.7610947, 0.7622144, 0.7633202, 0.76445603, 0.76554596, 0.7666071, 0.76763695, 0.76866996, 0.7697392, 0.77075976, 0.7717471, 0.77275103, 0.77376735, 0.77477527, 0.77574944, 0.7766875, 0.77762675, 0.77858436, 0.7795439, 0.78046364, 0.7814183, 0.78236413, 0.7833137, 0.78425646, 0.7851797, 0.7860629, 0.78700554, 0.7878816, 0.7887641, 0.7896225, 0.7905224, 0.7913667, 0.79217166, 0.79302156, 0.79384065, 0.7946832, 0.7954929, 0.79629797, 0.79709, 0.7978903, 0.7986618, 0.79943526, 0.800223, 0.80099213, 0.80173004, 0.8024925, 0.80324423, 0.8039714, 0.8047007, 0.8054172, 0.8061207, 0.80684406, 0.80755705, 0.8082645, 0.8089772, 0.80966866, 0.81036, 0.8110173, 0.8117051, 0.8123719, 0.81305164, 0.81370777, 0.81437, 0.81503576, 0.8156836, 0.81632286, 0.8169732, 0.8176119, 0.81824356, 0.8188832, 0.8194856, 0.82010233, 0.8207231, 0.8213295, 0.82194424, 0.82253975, 0.8231284, 0.8237271, 0.8243224, 0.824892, 0.82546633, 0.82604456, 0.82658154, 0.8271475, 0.82768524, 0.8282272, 0.82878923, 0.8293336, 0.82986945, 0.8304084, 0.8309435, 0.8314972, 0.8320025, 0.8325329, 0.8330708, 0.83360666, 0.834116, 0.83462054, 0.8351265, 0.835639, 0.83613473, 0.8366399, 0.8371073, 0.8376072, 0.838082, 0.8385617, 0.8390687, 0.8395288, 0.839988, 0.8404436, 0.8409142, 0.84137493, 0.8418515, 0.8423289, 0.8428098, 0.84327257, 0.84371734, 0.8441793, 0.84462905, 0.84508234, 0.8455234, 0.84597355, 0.8464227, 0.8468474, 0.84726703, 0.84770536, 0.8481276, 0.84855145, 0.8489636, 0.84938467, 0.84980446, 0.850223, 0.85065275, 0.85106593, 0.85147274, 0.8518803, 0.852254, 0.8526486, 0.85304123, 0.85342705, 0.8538069, 0.8541884, 0.854567, 0.85494655]
****************************************************
train cor: 85.535640
train loss: 0.027683
****************************************************
Test Correlation: 85.550910
Test loss: 0.057252