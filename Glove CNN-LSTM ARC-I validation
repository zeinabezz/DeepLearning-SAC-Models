#embedding in glove + multichannel cnn-pool-lstm +validation
from numpy.random import seed
seed(1)
from tensorflow import set_random_seed
set_random_seed(2)

from pickle import load
from keras.layers import Layer
from numpy import array
from keras.preprocessing.text import Tokenizer
from gensim.models import Word2Vec
from keras.preprocessing.sequence import pad_sequences
from numpy import asarray
from keras.utils.vis_utils import plot_model
from keras.models import Model
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from scipy.stats import pearsonr
from sklearn.metrics import mean_squared_error
import numpy as np
from keras.callbacks import EarlyStopping
from keras.layers import Input
from numpy import zeros
from keras.layers import Dense
from keras.layers import Flatten
from gensim.models import Word2Vec
from keras.layers import Dropout
from keras.layers import Embedding
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers.merge import concatenate
import tensorflow as tf
import numpy
from sklearn.model_selection import KFold
import pandas as pd
from nltk.tokenize import sent_tokenize
from keras import backend as k
from keras import optimizers
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer 
from tensorflow.python.keras import backend as K
import keras
from keras.layers import LSTM
from keras import backend as K
from numpy import asarray
from keras.callbacks import LearningRateScheduler
import numpy as np
import matplotlib.pyplot as plt
from keras.callbacks import EarlyStopping


def lr_schedule(epoch):
    lrate = 0.0007
    if epoch > 10:
        lrate = 0.0001      
    return lrate
    
def plot_history(lh, desc, min_val_loss):
    plt.plot(lh, label=desc)
    plt.title(desc)    
    plt.xlabel('epochs')    
    plt.ylabel(desc)    
    plt.legend()
    if(desc.find('validation')!=-1):
        plt.show()
    if(desc.find('validation loss')!=-1):
        min_loss=(np.where(lh == min(lh))[0][0])
        print('epoch with minium val_loss is: ', min_loss+1)
        print('validation loss minimum value is: ', min(lh))
        print('avg val_loss is: ', lh)
        return min_loss
    if(desc.find('validation correlation')!=-1):
        print('validation correlation value is: ', lh[min_val_loss])
    if(desc.find('training correlation')!=-1):
        print('training correlation value is: ', lh[min_val_loss])        
    print(desc)
    print(lh) 
    

def correlation_coefficient(y_true, y_pred):
   
    return tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true)[1]

def plot_histories(histories, desc, min_val_loss):
    min_hist=len(histories[0])
    for i in range(len(histories)):
        print('histories length is: ', len(histories))
        if (len(histories[i])<min_hist):
            print('history number: ', i, ' has: ', len(histories[i]), ' epochs')
            min_hist=len(histories[i])         
    print('min number of epochs: ', min_hist)
    lh=np.zeros(min_hist)
    for i in range (min_hist):
        for j in range(len(histories)):
            lh[i]=lh[i]+histories[j][i]
    lh=lh/len(histories)
    plt.plot(lh, label=desc)
    plt.title(desc)    
    plt.xlabel('epochs')    
    plt.ylabel(desc)    
    plt.legend()
    if(desc.find('validation')!=-1):
        plt.show()
    if(desc.find('validation loss')!=-1):
        min_loss=(np.where(lh == min(lh))[0][0])
        print('epoch with minium val_loss is: ', min_loss+1)
        print('validation loss minimum value is: ', min(lh))
        print('avg val_loss is: ', lh)
        return min_loss
    if(desc.find('validation correlation')!=-1):
        print('validation correlation value is: ', lh[min_val_loss])
    if(desc.find('training correlation')!=-1):
        print('training correlation value is: ', lh[min_val_loss])        
    print(desc)
    print(lh)


def construct_embedding():
    # load the whole embedding into memory
    embeddings_index = dict()
    f = open('../input/embedding/word_embedding50d.txt')
    for line in f:
        values = line.split()
        word = values[0]
        coefs = asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()
    print('Loaded %s word vectors.' % len(embeddings_index))
    return embeddings_index




# define the model
def define_model(length, vocab_size, embedding_matrix):
	# channel 1
    inputs1 = Input(shape=(length,))
    embedding1 = Embedding(vocab_size, 50, weights=[embedding_matrix],trainable=False)(inputs1)
    conv1 = Conv1D(filters=350, kernel_size=3, activation='relu')(embedding1)
    drop1 = Dropout(0.2)(conv1)
    nor11=keras.layers.BatchNormalization()(drop1)

    pool1 = MaxPooling1D(pool_size=10)(nor11)
    pdrop1 = Dropout(0.2)(pool1)
    nor12=keras.layers.BatchNormalization()(pdrop1)

    #ls1=LSTM(200)(nor12)
    #ldrop1 = Dropout(0.2)(ls1)
    #lnor1=keras.layers.BatchNormalization()(ldrop1)

	# channel 2
    inputs2 = Input(shape=(length,))
    embedding2 = Embedding(vocab_size, 50, weights=[embedding_matrix],trainable=False)(inputs2)
    conv2 = Conv1D(filters=350, kernel_size=4, activation='relu')(embedding2)
    drop2 = Dropout(0.2)(conv2)
    nor21=keras.layers.BatchNormalization()(drop2)

    pool2 = MaxPooling1D(pool_size=10)(nor21)
    pdrop2 = Dropout(0.2)(pool2) 
    nor22=keras.layers.BatchNormalization()(pdrop2)
    
    #ls2=LSTM(200)(nor22)
    #ldrop2 = Dropout(0.2)(ls2)
    #lnor2=keras.layers.BatchNormalization()(ldrop2)

    
	# channel 3
    inputs3 = Input(shape=(length,))
    embedding3 = Embedding(vocab_size, 50, weights=[embedding_matrix],trainable=False)(inputs3)
    conv3 = Conv1D(filters=350, kernel_size=5, activation='relu')(embedding3)
    drop3 = Dropout(0.2)(conv3)
    nor31=keras.layers.BatchNormalization()(drop3)

    pool3 = MaxPooling1D(pool_size=10)(nor31)
    pdrop3 = Dropout(0.2)(pool3) 
    nor32=keras.layers.BatchNormalization()(pdrop3)

    
  #  ls3=LSTM(250)(nor32)
  #  ldrop3 = Dropout(0.2)(ls3)
  #  lnor3=keras.layers.BatchNormalization()(ldrop3)
    

	# merge
    merged=concatenate([nor12, nor22, nor32])
    ls3=LSTM(250)(merged)
    ldrop3 = Dropout(0.2)(ls3)
    lnor3=keras.layers.BatchNormalization()(ldrop3)

    
    dense1 = Dense(100, activation='relu')(lnor3)
    nor4=keras.layers.BatchNormalization()(dense1)
    #dense2 = Dense(50, activation='elu')(nor4)
    #nor5=keras.layers.BatchNormalization()(dense2)

    outputs = Dense(1, activation='elu')(nor4)
    noroutputs=keras.layers.BatchNormalization()(outputs)
    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=noroutputs)
    # compile
    model.compile(loss='mse', optimizer=optimizers.Adam(lr=0.003), metrics=[correlation_coefficient])
    K.get_session().run(tf.local_variables_initializer())

    # summarize 
    print(model.summary())
    plot_model(model, show_shapes=True, to_file='multichannel.png')

    return model

# load a clean dataset
def load_dataset(filename):
    return load(open(filename, 'rb'))

#preprocessing text
def preprocess(lines):
	#print(lines)      
	ps = PorterStemmer() 
	for i in range(len(lines)):
		tokens = lines[i].split() 
        # filter out stop words then stem the remaining words
		stop_words = set(stopwords.words('english'))    
		tokens = [ps.stem(w) for w in tokens if not w in stop_words]    
		lines[i]=' '.join(tokens)  
	#print('lines: ')
	#print(lines)
	return lines

    

# encode a list of lines
def encode_text(tokenizer, lines, length):  
	# integer encode
	encoded = tokenizer.texts_to_sequences(lines)
	# pad encoded sequences
	padded = pad_sequences(encoded, maxlen=length, padding='post')
	return padded


# fit a tokenizer
def create_tokenizer(lines):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(lines)
    return tokenizer

# calculate the maximum document length
def max_length(lines):
    return max([len(s.split()) for s in lines])

# encode a list of lines
def encode_text(tokenizer, lines, length):
	# integer encode
    encoded = tokenizer.texts_to_sequences(lines)
	# pad encoded sequences
    padded = pad_sequences(encoded, maxlen=length, padding='post')
    return padded

def embed (vocab_size, embeddings_index, t):
    embedding_matrix = zeros((vocab_size, 50))
    for word, i in t.word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
    return embedding_matrix


# define training data
f = pd.read_csv('../input/saconcatenation/concatenationTrainingData3.csv', encoding='ISO-8859-1')
ftest = pd.read_csv('../input/saconcatenation/concatenationTestingData.csv', encoding='ISO-8859-1')
fvalid = pd.read_csv('../input/saconcatenation/concatenationValidationData3.csv', encoding='ISO-8859-1')
data_train = pd.DataFrame(data=f) 
trainLines=(data_train.answer).str.lower()
trainLines=preprocess(trainLines)
trainLabels=data_train.score

data_valid = pd.DataFrame(data=fvalid) 
validLines=(data_valid.answer).str.lower()
validLines=preprocess(validLines)
validLabels=data_valid.score

data_test = pd.DataFrame(data=ftest) 
testLines=(data_test.answer).str.lower()
testLines=preprocess(testLines)
testLabels=data_test.score


mergedLines = [trainLines , validLines, testLines]
allmerged = pd.concat(mergedLines)

# create tokenizer
tokenizer = create_tokenizer(allmerged.str.lower())


# calculate max document length
length = max_length(allmerged)

# calculate vocabulary size
vocab_size = len(tokenizer.word_index) + 1


print('Max answerlength: %d' % length)
print('Vocabulary size: %d' % vocab_size)

# encode data
alldataX = encode_text(tokenizer, allmerged, length)


tr=(trainLines.size)
te=(testLines.size)
v=(validLines.size)
trainX=alldataX[0:tr]
validX=alldataX[tr:tr+v]
testX=alldataX[tr+v:]

   
    
print(trainX.shape,  testX.shape)


embeddings_index=construct_embedding()
embedding_matrix=embed (vocab_size, embeddings_index, tokenizer)

traincvlossscores=[]
traincvscores=[]
cvlossscores=[]
cvscores=[]
histories=[]
val_histories=[]
cor_histories=[]
val_cor_histories=[]



	# define model
model = define_model(length, vocab_size, embedding_matrix)


lr_scheduler=LearningRateScheduler(lr_schedule)
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, min_delta=1e-5, patience=80)
mcp=keras.callbacks.ModelCheckpoint('bestweights4glovecnnlstm1', monitor='val_loss', save_best_only=True, verbose=1)
callbacks_list = [lr_scheduler, mcp, es] 

	# fit model
history=model.fit([trainX, trainX, trainX], array(trainLabels), epochs=300, batch_size=80, 
                  callbacks = callbacks_list,  validation_data=([validX, validX, validX],validLabels))
histories.append(history.history['loss'])
val_histories.append(history.history['val_loss'])
cor_histories.append(history.history['correlation_coefficient'])
val_cor_histories.append(history.history['val_correlation_coefficient'])

	# save the model
model.save('model.h5')
	# evaluate model on training dataset
loss, cor= model.evaluate([trainX,trainX, trainX], array(trainLabels), verbose=0)
print('train Correlation: %f' % (cor*100))
print('train loss: %f' % (loss*100))
traincvscores.append(cor*100)
traincvlossscores.append(loss*100)


    
	# evaluate model on validation dataset
loss, cor= model.evaluate([validX,validX, validX], array(validLabels), verbose=0)
print('validation Correlation: %f' % (cor*100))
print('validation loss: %f' % (loss*100))
cvscores.append(cor*100)
cvlossscores.append(loss*100)
#plot loss, validation for each train and validation fold
plot_history(history.history['loss'], 'training loss', 0)      
min_loss=plot_history(history.history['val_loss'], 'validation loss', 0)
plot_history(history.history['correlation_coefficient'], 'training correlation', min_loss)
plot_history(history.history['val_correlation_coefficient'], 'validation correlation', min_loss)   

    
print('***********************************')
print('train cor: %f' % numpy.mean(traincvscores))
print('train loss: %f' % numpy.mean(traincvlossscores))
print('***********************************')
print('cv cor: %f' % numpy.mean(cvscores))
print('cv loss: %f' % numpy.mean(cvlossscores))
print('************************************')

# evaluate model on test dataset dataset
loss, cor = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)
print('Test Correlation: %f' % (cor*100))
print('Test loss: %f' % (loss*100))