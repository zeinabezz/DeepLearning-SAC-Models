#multichannel cnn+concatenation+validation
from numpy.random import seed
seed(1)
from tensorflow import set_random_seed
set_random_seed(2)

from pickle import load
import pandas as pd
from keras import backend as K
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer 
from sklearn.model_selection import KFold
from numpy import array
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import EarlyStopping
from keras.utils.vis_utils import plot_model
from keras import optimizers
from numpy import array
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.layers import Embedding
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers.merge import concatenate
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy
import keras.backend as k
import keras

def plot_history(lh, desc, min_val_loss):
    plt.plot(lh, label=desc)
    plt.title(desc)    
    plt.xlabel('epochs')    
    plt.ylabel(desc)    
    plt.legend()
    plt.show()
    if(desc.find('loss')!=-1):
        min_loss=(np.where(lh == min(lh))[0][0])
        print('epoch with minium loss is: ', min_loss+1)
        print('training loss minimum value is: ', min(lh))
        print('avg loss is: ', lh)
        return min_loss
    if(desc.find('correlation')!=-1):
        print('correlation value is: ', lh[min_val_loss])
    print(desc)
    print(lh) 


    
def plot_histories(histories, desc):
    min_hist=len(histories[0])
    for i in range(len(histories)):
        print('histories length is: ', len(histories))
        if (len(histories[i])<min_hist):
            print('history number: ', i, ' has: ', len(histories[i]), ' epochs')
            min_hist=len(histories[i])         
    print('min number of epochs: ', min_hist)
    #for i in range(len(histories)):
    #     histories[i]=expand_list(histories[i], max_hist)
     #    print('history number: ', i, ' has: the following values: ')
      #   print(histories[i])
       #  print('finish')

    lh=np.zeros(min_hist)
    for i in range (min_hist):
        for j in range(len(histories)):
            lh[i]=lh[i]+histories[j][i]
    lh=lh/len(histories)
    plt.plot(lh, label=desc)
    plt.title(desc)    
    plt.xlabel('epochs')    
    plt.ylabel(desc)    
    plt.legend()
    
    plt.show()
    if(desc.find('loss')!=-1):
        print('epoch with minium loss is: ', (np.where(lh == min(lh))[0][0])+1)
        print('avg loss is: ', lh)

def correlation_coefficient(y_true, y_pred):
    return tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true)[1]
 
# load a clean dataset
def load_dataset(filename):
    return load(open(filename, 'rb'))

# fit a tokenizer
def create_tokenizer(lines):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(lines)
    return tokenizer

# calculate the maximum document length
def max_length(lines):
    return max([len(s.split()) for s in lines])

#preprocessing text
def preprocess(lines):
    #print(lines)      
    ps = PorterStemmer() 
    for i in range(len(lines)):
        tokens = lines[i].split() 
        # filter out stop words then stem the remaining words
        stop_words = set(stopwords.words('english'))    
        tokens = [ps.stem(w) for w in tokens if not w in stop_words]    
        lines[i]=' '.join(tokens)  
    #print('lines: ')
    #print(lines)
    return lines

# encode a list of lines
def encode_text(tokenizer, lines, length):  
    # integer encode
    encoded = tokenizer.texts_to_sequences(lines)
    # pad encoded sequences
    padded = pad_sequences(encoded, maxlen=length, padding='post')
    return padded

# define the model
def define_model(length, vocab_size):
    # channel 1
    inputs1 = Input(shape=(length,))
    embedding1 = Embedding(vocab_size, 300)(inputs1)
    conv1 = Conv1D(filters=300, kernel_size=3, activation='elu')(embedding1)
    nor11=keras.layers.BatchNormalization()(conv1)
    drop1 = Dropout(0.2)(nor11)
    pool1 = MaxPooling1D(pool_size=5)(drop1)
    nor12=keras.layers.BatchNormalization()(pool1)
    pdrop1 = Dropout(0.2)(nor12)
    flat1 = Flatten()(pdrop1)
    # channel 2
    inputs2 = Input(shape=(length,))
    embedding2 = Embedding(vocab_size, 300)(inputs2)
    conv2 = Conv1D(filters=300, kernel_size=4, activation='elu')(embedding2)
    nor21=keras.layers.BatchNormalization()(conv2)
    
    drop2 = Dropout(0.2)(nor21)
    pool2 = MaxPooling1D(pool_size=5)(drop2)
    nor22=keras.layers.BatchNormalization()(pool2)
    
    pdrop2 = Dropout(0.2)(nor22)   
    flat2 = Flatten()(pdrop2)
    # channel 3
    inputs3 = Input(shape=(length,))
    embedding3 = Embedding(vocab_size, 300)(inputs3)
    conv3 = Conv1D(filters=300, kernel_size=5, activation='elu')(embedding3)
    nor31=keras.layers.BatchNormalization()(conv3)

    
    drop3 = Dropout(0.2)(nor31)
    pool3 = MaxPooling1D(pool_size=5)(drop3)
    nor32=keras.layers.BatchNormalization()(pool3)

    
    pdrop3 = Dropout(0.2)(nor32)   
    flat3 = Flatten()(pdrop3)
    # merge
    merged = concatenate([flat1, flat2, flat3])
    # interpretation
    dense1 = Dense(200, activation='elu')(merged)
    nor5=keras.layers.BatchNormalization()(dense1)
    pdrop5 = Dropout(0.45)(nor5) 
    dense2 = Dense(100, activation='elu')(pdrop5)
    nor6=keras.layers.BatchNormalization()(dense2)
    pdrop6 = Dropout(0.45)(nor6) 
    
    outputs = Dense(1, activation='elu')(pdrop6)
#   outputs=keras.layers.BatchNormalization()(soutputs)

    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)
    # compile
#   adams=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
    model.compile(loss='mse', optimizer=optimizers.Adam(lr=0.001), metrics=[correlation_coefficient])
    # tensorflow variables need to be initialized before calling model.fit()
    K.get_session().run(tf.local_variables_initializer())
    # summarize
    print(model.summary())
    plot_model(model, show_shapes=True, to_file='multichannel.png')
    return model


# define training data
f = pd.read_csv('../input/saconcatenation/concatenationTrainingData3.csv', encoding='ISO-8859-1')
ftest = pd.read_csv('../input/saconcatenation/concatenationTestingData.csv', encoding='ISO-8859-1')
fvalid = pd.read_csv('../input/saconcatenation/concatenationValidationData3.csv', encoding='ISO-8859-1')
data_train = pd.DataFrame(data=f) 
trainLines=(data_train.answer).str.lower()
trainLines=preprocess(trainLines)
trainLabels=data_train.score

data_valid = pd.DataFrame(data=fvalid) 
validLines=(data_valid.answer).str.lower()
validLines=preprocess(validLines)
validLabels=data_valid.score

data_test = pd.DataFrame(data=ftest) 
testLines=(data_test.answer).str.lower()
testLines=preprocess(testLines)
testLabels=data_test.score

labels = [trainLabels, validLabels]

trainLabels = pd.concat(labels)
mergedLines = [trainLines , validLines, testLines]
allmerged = pd.concat(mergedLines)

# create tokenizer
tokenizer = create_tokenizer(allmerged.str.lower())


# calculate max document length
length = max_length(allmerged)

# calculate vocabulary size
vocab_size = len(tokenizer.word_index) + 1


print('Max answerlength: %d' % length)
print('Vocabulary size: %d' % vocab_size)

# encode data
alldataX = encode_text(tokenizer, allmerged, length)


tr=(trainLines.size)
te=(testLines.size)
v=(validLines.size)
trainX=alldataX[0:tr+v]

#validX=alldataX[tr:tr+v]
testX=alldataX[tr+v:]

   
    
print(trainX.shape,  testX.shape)


traincvlossscores=[]
traincvscores=[]
cvlossscores=[]
cvscores=[]
histories=[]
val_histories=[]
cor_histories=[]
val_cor_histories=[]
    # enumerate splits

    # define model
model = define_model(length, vocab_size)

mcp=keras.callbacks.ModelCheckpoint('bestweights1cnn300d', monitor='loss', save_best_only=True, verbose=1)
es = EarlyStopping(monitor='loss', mode='min', min_delta=1e-5, patience=80, verbose=1)
callbacks_list = [es, mcp]
    
    # fit model
history=model.fit([trainX,trainX,trainX], array(trainLabels), epochs=350, 
                      batch_size=80, callbacks = callbacks_list, validation_data=([testX, testX, testX],testLabels))
histories.append(history.history['loss'])
#val_histories.append(history.history['val_loss'])
cor_histories.append(history.history['correlation_coefficient'])
#val_cor_histories.append(history.history['val_correlation_coefficient'])
    # save the model
model.save('model.h5')
    # evaluate model on training dataset
loss, cor = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)
print('train Corelation: %f' % (cor*100))
print('train loss: %f' % (loss*100))
traincvscores.append(cor*100)
traincvlossscores.append(loss*100)

######

    #plot loss, validation for each train and validation fold
min_loss=plot_history(history.history['loss'], 'training loss', 0)      
#min_loss=plot_history(history.history['val_loss'], 'validation loss',0)
plot_history(history.history['correlation_coefficient'], 'training correlation', min_loss)
#plot_history(history.history['val_correlation_coefficient'], 'validation correlation', min_loss)  
print('****************************************************')
print('train cor: %f' % numpy.mean(traincvscores))
print('train loss: %f' % numpy.mean(traincvlossscores))
print('****************************************************')

# evaluate model on test dataset dataset
loss, cor = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)
print('Test Correlation: %f' % (cor*100))
print('Test loss: %f' % (loss*100))